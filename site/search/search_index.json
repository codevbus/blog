{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"about/","text":"About I'm a DevOps engineer based out of Grand Rapids, MI. I've worked with global-scale and startup companies, focusing on engineering ops, security, and automating the software lifecycle. I've been fully remote since 2017; it's an integral of my work/life balance and workflow. When it comes to engineering and software infrastructure, I prefer simplicity, modularity, and robustness. Complexity and brittleness are code smells. I also create technical content. Infrequently on the blog, often elsewhere. Right now I'm thinking about AI, OSS, and how to build simple things that are nice.","title":"About"},{"location":"about/#about","text":"I'm a DevOps engineer based out of Grand Rapids, MI. I've worked with global-scale and startup companies, focusing on engineering ops, security, and automating the software lifecycle. I've been fully remote since 2017; it's an integral of my work/life balance and workflow. When it comes to engineering and software infrastructure, I prefer simplicity, modularity, and robustness. Complexity and brittleness are code smells. I also create technical content. Infrequently on the blog, often elsewhere. Right now I'm thinking about AI, OSS, and how to build simple things that are nice.","title":"About"},{"location":"projects/","text":"","title":"Projects"},{"location":"publications/","text":"Publications This page will link to any of my published content on 3rd-party blogs, sites, etc... GitLab Observability vs. monitoring in DevOps How to Start a Great OSS Project A Cloud Guru How to test an LLM-based application as a DevOps engineer 5 ways to implement DevSecOps right now AWS IAM Security Best Practices 5 things we love about Terraform Top DevOps skills and technologies (and how to learn them) AWS adds to the no-code pile: Is it the end of the engineer? Comparing AWS, Azure, and Google Cloud IAM services NoSQL databases Comparison: Cosmos DB vs DynamoDB vs Cloud Datastore and Bigtable IAMONDEMAND(IOD) Jenkins and Spinnaker: Turbocharge Your CI/CD With Cloud Native Dev.to Deploy AWS Lambda functions with aws-sam-cli and Travis-CI Deploy AWS Lambda functions with aws-sam-cli and Travis-CI: Part 2 Track Your Dev.to Metrics with no-code Graph Your Dev.to Metrics with no-code SkilledInc How to Hire and Interview For DevOps Engineers Linuxserver.io How To Setup Duplicati As A Personal Backup Service Using Docker and AWS S3","title":"Publications"},{"location":"publications/#publications","text":"This page will link to any of my published content on 3rd-party blogs, sites, etc...","title":"Publications"},{"location":"publications/#gitlab","text":"Observability vs. monitoring in DevOps How to Start a Great OSS Project","title":"GitLab"},{"location":"publications/#a-cloud-guru","text":"How to test an LLM-based application as a DevOps engineer 5 ways to implement DevSecOps right now AWS IAM Security Best Practices 5 things we love about Terraform Top DevOps skills and technologies (and how to learn them) AWS adds to the no-code pile: Is it the end of the engineer? Comparing AWS, Azure, and Google Cloud IAM services NoSQL databases Comparison: Cosmos DB vs DynamoDB vs Cloud Datastore and Bigtable","title":"A Cloud Guru"},{"location":"publications/#iamondemandiod","text":"Jenkins and Spinnaker: Turbocharge Your CI/CD With Cloud Native","title":"IAMONDEMAND(IOD)"},{"location":"publications/#devto","text":"Deploy AWS Lambda functions with aws-sam-cli and Travis-CI Deploy AWS Lambda functions with aws-sam-cli and Travis-CI: Part 2 Track Your Dev.to Metrics with no-code Graph Your Dev.to Metrics with no-code","title":"Dev.to"},{"location":"publications/#skilledinc","text":"How to Hire and Interview For DevOps Engineers","title":"SkilledInc"},{"location":"publications/#linuxserverio","text":"How To Setup Duplicati As A Personal Backup Service Using Docker and AWS S3","title":"Linuxserver.io"},{"location":"posts/","text":"","title":"Blog"},{"location":"posts/deploy-aws-lambda-functions-with-aws-sam-cli-and-travis-ci-1/","text":"As serverless computing makes its way into the toolbox of more engineers and developers, the need for streamlined development and deployment options grows in importance. This 2-part series will share a basic workflow I've developed for developing and deploying serverless functions to AWS. This workflow is going to employ the largely ubiquitous, free platforms offered by GitHub and Travis-CI for version-control and CI, respectively, but it could be adapted to a wider variety of VCS and CI/CD platforms. This will be a 2-part article. The first part will end with us having a local development environment with the aws-sam-cli tool, as well as a basic, working example of an AWS Lambda function. First, let's look at the prerequisites needed. Prerequisites This series is going to assume some basic knowledge around topics like AWS, and serverless functions. Ideally, the reader has a few AWS Lambda functions under their belt already, and is looking for a more streamlined and automated way to deploy them. It's also assumed there is some basic familiarity and experience with version control systems, specifically Git/Github in this case. This workflow was performed on a Macbook Pro running OSX 10.12(Sierra). Installation of tools for different platforms such as Linux or Windows may vary slightly. The tools and services you'll need are as follows: An AWS account you have permission to programmatically deploy resources to An account on GitHub.com An account on Travis-CI.org The latest Python 2.7 Docker CE An up-to-date install of the aws-sam-cli tool Note that the aws-sam-cli tool, a key part of the workflow, requires local, valid AWS user credentials. These are typically deployed to a workstation as part of the installation of the AWS CLI tool. Go ahead and follow these instructions if you don't already have this set up: Installation Guide Configuration Guide If you're having trouble installing any of the tools mentioned, check the documentation or GitHub issues for the given project. Feel free to post in the comments as well and I'll do my best to help out. DISCLAIMER : Any AWS resources you provision during the course of this tutorial may result in charges being billed to your account. The author assumes no responsibility, legal or otherwise, for charges incurred. What Are We Doing? Why? I touched on it briefly in the introduction to the article: serverless computing is booming. It's easy to see the attractiveness presented by the platform. No more managing servers, OS updates, and firewall rules. Discrete, ephemeral, finite chunks of computing resources that can be activated on demand. Why pay for a server instance to sit and run a script every 24 hours when a serverless function can do the same for a fraction of the cost and maintenance overhead? However, with new usage models comes new challenges. How do you develop and test locally? How do make serverless development part of your CI/CD workflow? The first question is easy: the engineers at AWS have developed a great tool with aws-sam-cli . It offers an incredibly streamlined and easy to way to develop, test, and provision AWS Lambda functions. Provisioning of live resources occurs via SAM and AWS CloudFormation. What about the second part? What if you want to go a step further, and automate testing and deployment of your functions? Enter Travis-CI . If you're not familiar, Travis-CI provides a free continuous integration and testing tool that is both powerful and easy to use. As I mentioned before, this is going to be a pretty simplistic workflow, but the basic concepts demonstrated here could be ported to a variety of different VCS and CI/CD tools as needed. For the serverless function, we'll be deploying some basic Python code that generates a list of active EC2 instances. I'll provide a link to the complete source code via GitHub if you want to use that as an example, but feel free to port your own function code into this tutorial. Let's get to creating our development environment and our function. Create a GitHub Repository First thing's first: we'll need to create a GitHub repository to store our project code, as well as for integration with Travis-CI. I've created my repo here . If you've already got a GitHub and Travis-CI account set up, great! If not, don't worry just yet about setting it up, we'll touch on that a little bit later. Create the Lambda Function If you want to use the example Lambda function I created, download this folder to your working directory. You can view the code inline below. The __init__.py file is empty and used to indicate to Python that the directory contains packages. import boto3 import json import datetime client = boto3.client('ec2') response = client.describe_instances( Filters=[ { 'Name': 'instance-state-name', 'Values': [ 'running' ] } ], MaxResults=100 ) def get_id(instances): \"\"\" returns a list of EC2 instances that are in the 'running' state, return object is a list of dicts \"\"\" running_instances = [] for instance in instances[\"Reservations\"][0][\"Instances\"]: running_instances.append({'Instance ID': instance['InstanceId']}) if not running_instances: print(\"No running instances detected!\") else: return running_instances def dt_converter(d): \"\"\" converts datetime objects to string objects \"\"\" if isinstance(d, datetime.datetime): return d.__str__() def encoder(j): \"\"\" dumps json objects to strings so any datetime objects can be converted to compatible strings before reloading as json again \"\"\" return json.loads(json.dumps(j, default = dt_converter)) def my_handler(event, context): \"\"\" lambda event handler that returns the list of instances \"\"\" return encoder(get_id(response)) This is a very simplistic function, with no error handling. The main purpose here is to demonstrate the potential of using CI/CD tools to streamline serverless development. Create the SAM template The sam local toolkit requires a SAM template to be defined. The template is a YAML file that is used to define certain aspects of how your serverless infrastructure will function. Additional documentation on usage is available on the aws-sam-cli repo . The file is available to download in the repo mentioned above. Code is also provided inline below: AWSTemplateFormatVersion: '2010-09-09' Transform: AWS::Serverless-2016-10-31 Description: Lists active EC2 instances Resources: GetInstances: Type: AWS::Serverless::Function Properties: Handler: getinstances.getinstances.my_handler Timeout: 300 Runtime: python3.6 AWS provides good documentation on the various components of a SAM template here . I'll cover the relevant portions of the template below: Resources: - Top level designation of the \"resources\" we want to provision with the template. In this case, it's simply a Lambda function. GetInstances: - The function name. This is referenced when using the sam tool to invoke the function. Type: - The type of resource. In this case AWS::Serverless::Function . Properties: - Denotes subsequent property values for the current resource layer. Handler: - This is where the magic happens. The \"Handler\" property points to the actual module containing the code of your function. Working from right to left: my_handler - The name of the event handler in the code itself. getinstances - Refers to the module/file getinstances.py . getinstances - The left-most index, it refers to the folder the module code is contained in. Timeout: - An integer value, in seconds, that defines how long the function can wait before a timeout occurs. Runtime: - Defines the runtime environment of the function code. In this case I've chosen python3.6 . Next, we'll generate a sample event payload to trigger the Lambda function with. Generate a Sample Payload In the case of AWS, Lambda functions are often triggered via some type of \"event\". A variety of AWS services generate events that are valid for triggering a function. Event payloads are JSON data structures that provide specific information about their origin and purpose. The aws-sam-cli tool provides an awesome feature that enables users to generate mock event payloads for local development and testing. For the purposes of this article, we'll generate a mock event from Cloudwatch Scheduled Events. These events function similar to the Linux cron system, and fire on pre-defined schedule values. Run the following command in the working directory of your repo: sam local generate-event schedule > event.json The output is saved in the event.json file, so that it can be re-used for function invocation as needed. At this point, this is what the folder structure of my project repo looks like: get-active-ec2-instances |-- README.md |-- event.json |-- getinstances | |-- __init__.py | `-- getinstances.py `-- template.yaml Your file or folder names may be slightly different, but at the very least you should have the module code, a template file, and a mock event. If everything looks good, let's move on to testing the Lambda function. Test The Lambda Function My favorite feature of the sam tool is the ability to invoke functions locally. No more uploading functions to AWS and testing them live to see if they work correctly. Run the following command in the working directory of your repo: sam local invoke \"GetInstances\" -e event.json If this is your first invocation with this particular runtime, the tool will take some time to download the Docker image from the remote repo. After the tool reports the request ID, and invocation billing data, the output of your function will be provided(if it generates readable output). In this case, the sample code generates a list of active EC2 instances, which would appear like so: [{\"Instance ID\": \"i-0123456789abcdef\"}, {\"Instance ID\": \"i-1011121314ghijklm\"}...] If the code does not detect any running instances, you will get this output instead: No running instances detected! If you encounter any errors, check the AWS documentation for common failure cases. I tend to find that most of my errors stem from authentication/credential issues. Feel free to post in the comments if you are struggling. What's Next Now we've got a great local development environment set up for developing and deploying Serverless functions to AWS. But we're not done. In part 2, we're going to use the Travis-CI tool to create an integrated testing and deployment set up for our Lambda function. Stay tuned!","title":"deploy aws lambda functions with aws-sam-cli and travis-ci: part 1"},{"location":"posts/deploy-aws-lambda-functions-with-aws-sam-cli-and-travis-ci-1/#prerequisites","text":"This series is going to assume some basic knowledge around topics like AWS, and serverless functions. Ideally, the reader has a few AWS Lambda functions under their belt already, and is looking for a more streamlined and automated way to deploy them. It's also assumed there is some basic familiarity and experience with version control systems, specifically Git/Github in this case. This workflow was performed on a Macbook Pro running OSX 10.12(Sierra). Installation of tools for different platforms such as Linux or Windows may vary slightly. The tools and services you'll need are as follows: An AWS account you have permission to programmatically deploy resources to An account on GitHub.com An account on Travis-CI.org The latest Python 2.7 Docker CE An up-to-date install of the aws-sam-cli tool Note that the aws-sam-cli tool, a key part of the workflow, requires local, valid AWS user credentials. These are typically deployed to a workstation as part of the installation of the AWS CLI tool. Go ahead and follow these instructions if you don't already have this set up: Installation Guide Configuration Guide If you're having trouble installing any of the tools mentioned, check the documentation or GitHub issues for the given project. Feel free to post in the comments as well and I'll do my best to help out. DISCLAIMER : Any AWS resources you provision during the course of this tutorial may result in charges being billed to your account. The author assumes no responsibility, legal or otherwise, for charges incurred.","title":"Prerequisites"},{"location":"posts/deploy-aws-lambda-functions-with-aws-sam-cli-and-travis-ci-1/#what-are-we-doing-why","text":"I touched on it briefly in the introduction to the article: serverless computing is booming. It's easy to see the attractiveness presented by the platform. No more managing servers, OS updates, and firewall rules. Discrete, ephemeral, finite chunks of computing resources that can be activated on demand. Why pay for a server instance to sit and run a script every 24 hours when a serverless function can do the same for a fraction of the cost and maintenance overhead? However, with new usage models comes new challenges. How do you develop and test locally? How do make serverless development part of your CI/CD workflow? The first question is easy: the engineers at AWS have developed a great tool with aws-sam-cli . It offers an incredibly streamlined and easy to way to develop, test, and provision AWS Lambda functions. Provisioning of live resources occurs via SAM and AWS CloudFormation. What about the second part? What if you want to go a step further, and automate testing and deployment of your functions? Enter Travis-CI . If you're not familiar, Travis-CI provides a free continuous integration and testing tool that is both powerful and easy to use. As I mentioned before, this is going to be a pretty simplistic workflow, but the basic concepts demonstrated here could be ported to a variety of different VCS and CI/CD tools as needed. For the serverless function, we'll be deploying some basic Python code that generates a list of active EC2 instances. I'll provide a link to the complete source code via GitHub if you want to use that as an example, but feel free to port your own function code into this tutorial. Let's get to creating our development environment and our function.","title":"What Are We Doing? Why?"},{"location":"posts/deploy-aws-lambda-functions-with-aws-sam-cli-and-travis-ci-1/#create-a-github-repository","text":"First thing's first: we'll need to create a GitHub repository to store our project code, as well as for integration with Travis-CI. I've created my repo here . If you've already got a GitHub and Travis-CI account set up, great! If not, don't worry just yet about setting it up, we'll touch on that a little bit later.","title":"Create a GitHub Repository"},{"location":"posts/deploy-aws-lambda-functions-with-aws-sam-cli-and-travis-ci-1/#create-the-lambda-function","text":"If you want to use the example Lambda function I created, download this folder to your working directory. You can view the code inline below. The __init__.py file is empty and used to indicate to Python that the directory contains packages. import boto3 import json import datetime client = boto3.client('ec2') response = client.describe_instances( Filters=[ { 'Name': 'instance-state-name', 'Values': [ 'running' ] } ], MaxResults=100 ) def get_id(instances): \"\"\" returns a list of EC2 instances that are in the 'running' state, return object is a list of dicts \"\"\" running_instances = [] for instance in instances[\"Reservations\"][0][\"Instances\"]: running_instances.append({'Instance ID': instance['InstanceId']}) if not running_instances: print(\"No running instances detected!\") else: return running_instances def dt_converter(d): \"\"\" converts datetime objects to string objects \"\"\" if isinstance(d, datetime.datetime): return d.__str__() def encoder(j): \"\"\" dumps json objects to strings so any datetime objects can be converted to compatible strings before reloading as json again \"\"\" return json.loads(json.dumps(j, default = dt_converter)) def my_handler(event, context): \"\"\" lambda event handler that returns the list of instances \"\"\" return encoder(get_id(response)) This is a very simplistic function, with no error handling. The main purpose here is to demonstrate the potential of using CI/CD tools to streamline serverless development.","title":"Create the Lambda Function"},{"location":"posts/deploy-aws-lambda-functions-with-aws-sam-cli-and-travis-ci-1/#create-the-sam-template","text":"The sam local toolkit requires a SAM template to be defined. The template is a YAML file that is used to define certain aspects of how your serverless infrastructure will function. Additional documentation on usage is available on the aws-sam-cli repo . The file is available to download in the repo mentioned above. Code is also provided inline below: AWSTemplateFormatVersion: '2010-09-09' Transform: AWS::Serverless-2016-10-31 Description: Lists active EC2 instances Resources: GetInstances: Type: AWS::Serverless::Function Properties: Handler: getinstances.getinstances.my_handler Timeout: 300 Runtime: python3.6 AWS provides good documentation on the various components of a SAM template here . I'll cover the relevant portions of the template below: Resources: - Top level designation of the \"resources\" we want to provision with the template. In this case, it's simply a Lambda function. GetInstances: - The function name. This is referenced when using the sam tool to invoke the function. Type: - The type of resource. In this case AWS::Serverless::Function . Properties: - Denotes subsequent property values for the current resource layer. Handler: - This is where the magic happens. The \"Handler\" property points to the actual module containing the code of your function. Working from right to left: my_handler - The name of the event handler in the code itself. getinstances - Refers to the module/file getinstances.py . getinstances - The left-most index, it refers to the folder the module code is contained in. Timeout: - An integer value, in seconds, that defines how long the function can wait before a timeout occurs. Runtime: - Defines the runtime environment of the function code. In this case I've chosen python3.6 . Next, we'll generate a sample event payload to trigger the Lambda function with.","title":"Create the SAM template"},{"location":"posts/deploy-aws-lambda-functions-with-aws-sam-cli-and-travis-ci-1/#generate-a-sample-payload","text":"In the case of AWS, Lambda functions are often triggered via some type of \"event\". A variety of AWS services generate events that are valid for triggering a function. Event payloads are JSON data structures that provide specific information about their origin and purpose. The aws-sam-cli tool provides an awesome feature that enables users to generate mock event payloads for local development and testing. For the purposes of this article, we'll generate a mock event from Cloudwatch Scheduled Events. These events function similar to the Linux cron system, and fire on pre-defined schedule values. Run the following command in the working directory of your repo: sam local generate-event schedule > event.json The output is saved in the event.json file, so that it can be re-used for function invocation as needed. At this point, this is what the folder structure of my project repo looks like: get-active-ec2-instances |-- README.md |-- event.json |-- getinstances | |-- __init__.py | `-- getinstances.py `-- template.yaml Your file or folder names may be slightly different, but at the very least you should have the module code, a template file, and a mock event. If everything looks good, let's move on to testing the Lambda function.","title":"Generate a Sample Payload"},{"location":"posts/deploy-aws-lambda-functions-with-aws-sam-cli-and-travis-ci-1/#test-the-lambda-function","text":"My favorite feature of the sam tool is the ability to invoke functions locally. No more uploading functions to AWS and testing them live to see if they work correctly. Run the following command in the working directory of your repo: sam local invoke \"GetInstances\" -e event.json If this is your first invocation with this particular runtime, the tool will take some time to download the Docker image from the remote repo. After the tool reports the request ID, and invocation billing data, the output of your function will be provided(if it generates readable output). In this case, the sample code generates a list of active EC2 instances, which would appear like so: [{\"Instance ID\": \"i-0123456789abcdef\"}, {\"Instance ID\": \"i-1011121314ghijklm\"}...] If the code does not detect any running instances, you will get this output instead: No running instances detected! If you encounter any errors, check the AWS documentation for common failure cases. I tend to find that most of my errors stem from authentication/credential issues. Feel free to post in the comments if you are struggling.","title":"Test The Lambda Function"},{"location":"posts/deploy-aws-lambda-functions-with-aws-sam-cli-and-travis-ci-1/#whats-next","text":"Now we've got a great local development environment set up for developing and deploying Serverless functions to AWS. But we're not done. In part 2, we're going to use the Travis-CI tool to create an integrated testing and deployment set up for our Lambda function. Stay tuned!","title":"What's Next"},{"location":"posts/deploy-aws-lambda-with-travisci-2/","text":"In part 1 of this series, we created a basic serverless function with AWS Lambda. In part 2, we're going to use GitHub and Travis-CI to set up a continuous integration workflow. If you completed part 1 successfully, you should have a working AWS Lambda function. However, it's only exists locally on the machine it was developed on. What if we want to deploy it to an AWS account? With an S3 bucket available, we could simply use sam package and sam deploy to create a deployment package for, and deploy our Lambda function to AWS. Boom. Done. That's not really sufficient for modern software/infrastructure deployment though, is it? Testing and validation is(or at least should be) a critical part of any deployment. How a system will function in a live environment should be determined before it ever hits that environment. It may not be possible to 100% simulate every scenario, or validate every piece of logic, but modern tools help us get close. Prerequisites It's assumed that we've got all the prerequisites from part 1, as well as the working function ready for deployment. It will probably be helpful to have a quick read of the introductory Travis-CI docs: Core Concepts For Beginners - Great intro on core concepts of CI and Travis. Getting Started Guide - Getting started using Travis. It might also be helpful to install the Travis gem. If you don't have Ruby gems, start here: https://rubygems.org/ . You can then do gem install travis to have a local cli utility available as needed as you work with Travis-CI. Let's go ahead and get started. What Are We Doing? Why?(cont.) Continuing the theme of part 1, we're setting up a CI testing and deployment workflow for serverless functions using AWS Lambda, Github, and Travis-CI. In part 2, we'll check in our code to the Github repo we've got set up. Then we're going to be setting up a couple AWS resources for Travis-CI and sam to use for deployment. Finally, we'll get everything configured and integrated together, so we've got an automated, CI deployment system! Commit The Code To Github If you haven't already, you should commit your code to Github. You should already have a remote repo configured. Github provides an excellent tutorial for getting set up if you need guidance. A key concept to remember(discussed in the Core Concepts documentation from Travis-CI) is that continuous integration(CI) is based on small code changes and frequent code check-ins. In fact, Travis-CI will run a build/deploy on every code check-in. This probably seems foreign, or even scary to someone used to more traditional development and deployment methodologies, but fear not! Once gotten used to, the paradigm of frequent deploys and frequent check-ins will become second nature, and ultimately allows faster feedback and iteration on code changes. Now we can move on to getting our AWS resources squared away so Travis-CI can run successful deployments. Deploy Your AWS Resources Before we can utilize Travis-CI to push our deployments, we need to setup three pieces of AWS infrastructure. Specifically, we'll be deploying an S3 bucket, an IAM user, and an IAM role. The S3 bucket will be used to upload code artifacts to via aws-sam-cli. The IAM user will be utilized by Travis-CI to interact with and create AWS resources, and the IAM role will be used by Lambda to query EC2 resources. I'm a big proponent of infrastructure as code, and I'd generally reach for something like Terraform or Cloudformation to deploy these resources. However , I want to keep this simple and focused on the CI integration, so we'll deploy these resources manually via the AWS console. Log in to your AWS account. Start with creating a basic S3 bucket with default options. Choose a unique name and save it for later. Next, we need to set up an IAM role. IAM roles are similar to IAM users, except they are designed, generally, to be used/assumed by services and non-human entities. They are an incredibly useful way to delegate permissions/access to your applications and services without having to store traditional credentials. In this specific case, the IAM role will be assumed by the AWS Lambda service so that your function has the neccessary permissions to carry out whatever actions it needs to. The permissions that you attach to the role will vary depending on the purpose of the function. If you're using the sample function I've provided, then some basic \"Read\" permissions on EC2 resources will be suffice. Go to the IAM console, and create a new role. Choose \"Lambda\" for the service. For the policy, AWS actually provides an existing policy we can use. Choose \"AmazonEC2ReadOnlyAccess\". On the \"Review\" screen, ensure everything looks correct. Give your a function a name that at least alludes to its primary function. I named mine ReadEC2StatusLambda . Be sure to save the role ARN for later, as you'll need it for the IAM user policy. Finally, we need to create an IAM user that Travis-CI can utilize to integrate with AWS. From the IAM console, you will need to navigate to \"Users\", where you view existing users, as well as create a new one. Go ahead and hit the \"Add User\" button. Start by giving the user a meaningful name that will make it easy to identify its purpose, like: TravisServerlessDeploymentUser . This will be a \"service account\", so for \"Access Type\", we're only going to choose \"programmatic access\". For permissions, choose \"Attach existing policies directly\", then hit the \"Create policy\" button. Choose the \"JSON\" tab, and enter the following into the policy editor: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"AllowListArtifactBucket\", \"Effect\": \"Allow\", \"Action\": \"s3:ListBucket\", \"Resource\": \"arn:aws:s3:::<your-bucket-name-here>\" }, { \"Sid\": \"AllowArtifactBucketUse\", \"Effect\": \"Allow\", \"Action\": [ \"s3:PutObjectAcl\", \"s3:PutObject\", \"s3:GetObjectAcl\", \"s3:GetObject\", \"s3:DeleteObject\" ], \"Resource\": \"arn:aws:s3:::<your-bucket-name-here>/*\" }, { \"Sid\": \"AllowLambdaAll\", \"Effect\": \"Allow\", \"Action\": \"lambda:*\", \"Resource\": \"*\" }, { \"Sid\": \"AllowCloudFormationAll\", \"Effect\": \"Allow\", \"Action\": \"cloudformation:*\", \"Resource\": \"*\" }, { \"Sid\": \"AllowIAMListPolicies\", \"Effect\": \"Allow\", \"Action\": \"iam:ListPolicies\", \"Resource\": \"*\" }, { \"Sid\": \"AllowPassLambdaRole\", \"Effect\": \"Allow\", \"Action\": \"iam:PassRole\", \"Resource\": \"<your-role-arn-here>\" } ] } Give the policy a meaningful name, similar to the IAM user you created previously. Be sure to replace both instances of <your-bucket-name-here> with the name of the bucket you created previously. You will also need to replace <your-role-arn-here> with the ARN of the role you generated in the previous step. Review the policy and save it. Afterwards you'll be returned to the user creation process. Hit \"refresh\" on the policy list, and search for the policy you just created. Select the policy, and proceed to \"Review\". If everything looks good, click \"Create User\". On the next screen, you'll be shown the access key and secret key. Make sure you save these in a safe place: this will be the only time you are shown them, and we'll need them for Travis-CI integration. Now we can move on to configuring our Travis-CI integration. Configure Travis-CI Integration In Part 1 of this series, some basic prerequisites were laid out, including an account on both GitHub and Travis-CI. Signing in to Travis-CI with your GitHub account, via OAuth, should make all your repositories available for Travis to integrate with. Check the Getting Started documentation if you need help. Once you get logged in to Travis-CI, you'll need to enable it for the specific respository you're trying to integrate with. Under your \"Profile\", it will list all your public repositories. If you want to integrate with private repositories, it requires a paid subscription on their .com site. Once that's complete, you now have working Travis-CI integration! The central component required for Travis-CI is the config file: .travis-ci.yml . This is a YAML dotfile that provides a variety of configuration options, which tell Travis \"how\" to build your software. Rather than provide an exhaustive dive on every possible option(of which there are many ), I'll provide the basic file I used for this project, and explain the relevant bits. I highly suggest taking the time to go through the official Travis-CI documentation to familiarize yourself with the options available, as well as the high-level concepts of how it operates. My configuration file is as follows: language: python python: - '2.7' branches: only: master install: - pip install awscli - pip install aws-sam-cli script: - sam validate - sam package --template-file template.yaml --s3-bucket <your-bucket-name> --output-template-file packaged.yaml deploy: provider: script script: sam deploy --template-file packaged.yaml --stack-name travis-serverless-test skip_cleanup: true on: branch: master notifications: email: on_failure: always env: global: - AWS_DEFAULT_REGION=us-east-2 - secure: <obfuscated AWS Access Key> - secure: <obfuscated AWS Secret Key> Let's take a walk through the configuration: language: python The language setting specifies the build environment that Travis uses for the project. Since we need to install awscli and aws-sam-cli via pip, Python is the natural choice. python: - '2.7' Defines what version of Python the project uses. In this case, due to a dependency with aws-sam-cli , we need Python 2. branches: only: master This setting ensures that builds are only triggered on commits to the \"master\" branch of the repo. install: - pip install awscli - pip install aws-sam-cli The \"install\" is the step where we install the packages/dependencies needed for our build. In this example, we use the Pip tool to install awscli and aws-sam-cli . script: - sam validate - sam package --template-file template.yaml --s3-bucket <your-bucket-name> --output-template-file packaged.yaml The \"script\" block is where the default build steps can be overridden. In this case, we call the sam validate command from the aws-sam-cli package to validate our SAM template, then sam package to build our function into a deployable artifact. Note that <your-bucket-name> should be replaced with the name of the S3 bucket that was created earlier in the tutorial. This is where the packaged deployment artifact is stored. Also note the --output-template-file packaged.yaml . This is a YAML file that will be utilized by the deployment step. deploy: provider: script script: sam deploy --template-file packaged.yaml --stack-name travis-serverless-test skip_cleanup: true on: branch: master The \"deploy\" step is where we actually deploy our code to whatever provider we've defined. Travis-CI supports many different providers directly: https://docs.travis-ci.com/user/deployment/#Supported-Providers . In this case we're using the generic script provider, which passes whatever build artifacts we generated in our build step to custom scripts or commands. Our command is sam deploy , which references the packaged.yaml artifact from the build step, as well as defines the name of the \"stack\". I've chosen travis-serverless-test , but feel free to employ whatever naming scheme you choose. This will actually be the name of a CloudFormation stack that the sam tool creates to deploy the Lambda function. skip_cleanup: true prevents Travis-CI from deleting any changes made during the build. on: branch: master ensures that deployments only occur on changes to the master branch. A possible configuration would be to allow builds on all branches, but deploys only on \"master\". notifications: email: on_failure: always Notifications defines how you are notified when things occur during the course of a build/deployment. In this example I've configured it notify via email whenever there is a failure. env: global: - AWS_DEFAULT_REGION=<your-aws-region> - secure: <obfuscated AWS Access Key> - secure: <obfuscated AWS Secret Key> The \"env\" block is where environment variables are defined. These are variables that can be made available for any stage of the process. I highly recommend reading the official docs: https://docs.travis-ci.com/user/environment-variables/ . Environment variables are a critical component of most CI systems, and familiarity with them will be a benefit going forward. For this particular use case, we need 3 specific environment variables defined: AWS_DEFAULT_REGION - Defines which region we are deploying resources to, and is required by the aws-sam-cli tool. I use us-east-2 . You'll want to choose the nearest region that has the Lambda and CloudFormation services available. secure - These are secure(encrypted) variables. In this particular case, we're using them to store the credentials of the IAM user we created previously. Again, the Travis-CI documentation: https://docs.travis-ci.com/user/environment-variables/#Defining-encrypted-variables-in-.travis.yml provides a great resource to explain how. In this case, I used the travis gem to add encrypted variables to my .travis-ci.yml file. If you're worried about sensitive credentials being stored in your shell history, most shells will not write a command to the hist file if you prepend them with a space. You can also add the variables via the Travis-CI site, where they will be obfuscated. However they will show up in the deployment log. I highly recommend using the gem method described above. Now Travis-CI is configured, and we're ready to deploy our code. Deploy Your Function Your project directory should look fairly similar to this(not counting .git* files): get-active-ec2-instances |-- .travis.yml |-- README.md |-- event.json |-- getinstances | |-- __init__.py | `-- getinstances.py `-- template.yaml If it looks slightly different, that's ok. As long as you have, at a bare minimum, a template.yaml and a .travis-ci.yml file, as well as your function, you should be good to go. At this point, if you commit and push your code to GitHub, it should trigger a job in Travis-CI. If you navigate to the Travis-CI site, the UI should show your repository and job status. Pay attention to the \"Job Log\", as that will show any errors or relevant output from the job. If everything went well: Congratulations! You just did a CI deployment of a serverless function! If you get any errors, check the output of the log for clues as to the root cause. Travis-CI provides some documentation on common failures as well . The last step we'll do is verify our Lambda function deployed correctly, via the AWS console. Verify It Works Now it's time to make sure everything worked as it should have. Log in to your AWS account console, and navigate to the \"Lambda\" service under \"Compute\". Make sure you're in the correct region(top right). You should see your function. If the name has additional alphanumeric characters appended to it, that's perfectly normal. The sam tool appends a unique ID to every function/stack it deploys. Click on your function. At the top of the interface, you'll see a \"Test\" button next to a drop-down menu that says Select a test event.. Test events provide simulated JSON payloads from various events in AWS that are used to trigger Lambda functions. Go ahead and choose \"Scheduled Event\" from the \"Event Template\" drop down. Name your event something meaningful like \"TestServerlessEvent\". Once you've saved the event, click \"Test\". Depending on your function, you'll see the output generated in the logs, or sent somewhere. In the case of the example provided in the tutorial, it should show a JSON block listing active EC2 instances. Wrap Up I'm hopeful this article series has provided at least a foundation for everyone to start thinking about ways to streamline serverless development and deployment. Some potential ideas to build on this project: Use the built-in Travis-CI Lambda provider. Add an API gateway to the project. Create a more complex commit/merge workflow for builds. Add unit-testing and code-coverage to builds. Add integration and functional testing to verify deployments. Comments? Questions? Issues? Please post in the comments. Until next time!","title":"deploy aws lambda functions with aws-sam-cli and travis-ci: part 2"},{"location":"posts/deploy-aws-lambda-with-travisci-2/#prerequisites","text":"It's assumed that we've got all the prerequisites from part 1, as well as the working function ready for deployment. It will probably be helpful to have a quick read of the introductory Travis-CI docs: Core Concepts For Beginners - Great intro on core concepts of CI and Travis. Getting Started Guide - Getting started using Travis. It might also be helpful to install the Travis gem. If you don't have Ruby gems, start here: https://rubygems.org/ . You can then do gem install travis to have a local cli utility available as needed as you work with Travis-CI. Let's go ahead and get started.","title":"Prerequisites"},{"location":"posts/deploy-aws-lambda-with-travisci-2/#what-are-we-doing-whycont","text":"Continuing the theme of part 1, we're setting up a CI testing and deployment workflow for serverless functions using AWS Lambda, Github, and Travis-CI. In part 2, we'll check in our code to the Github repo we've got set up. Then we're going to be setting up a couple AWS resources for Travis-CI and sam to use for deployment. Finally, we'll get everything configured and integrated together, so we've got an automated, CI deployment system!","title":"What Are We Doing? Why?(cont.)"},{"location":"posts/deploy-aws-lambda-with-travisci-2/#commit-the-code-to-github","text":"If you haven't already, you should commit your code to Github. You should already have a remote repo configured. Github provides an excellent tutorial for getting set up if you need guidance. A key concept to remember(discussed in the Core Concepts documentation from Travis-CI) is that continuous integration(CI) is based on small code changes and frequent code check-ins. In fact, Travis-CI will run a build/deploy on every code check-in. This probably seems foreign, or even scary to someone used to more traditional development and deployment methodologies, but fear not! Once gotten used to, the paradigm of frequent deploys and frequent check-ins will become second nature, and ultimately allows faster feedback and iteration on code changes. Now we can move on to getting our AWS resources squared away so Travis-CI can run successful deployments.","title":"Commit The Code To Github"},{"location":"posts/deploy-aws-lambda-with-travisci-2/#deploy-your-aws-resources","text":"Before we can utilize Travis-CI to push our deployments, we need to setup three pieces of AWS infrastructure. Specifically, we'll be deploying an S3 bucket, an IAM user, and an IAM role. The S3 bucket will be used to upload code artifacts to via aws-sam-cli. The IAM user will be utilized by Travis-CI to interact with and create AWS resources, and the IAM role will be used by Lambda to query EC2 resources. I'm a big proponent of infrastructure as code, and I'd generally reach for something like Terraform or Cloudformation to deploy these resources. However , I want to keep this simple and focused on the CI integration, so we'll deploy these resources manually via the AWS console. Log in to your AWS account. Start with creating a basic S3 bucket with default options. Choose a unique name and save it for later. Next, we need to set up an IAM role. IAM roles are similar to IAM users, except they are designed, generally, to be used/assumed by services and non-human entities. They are an incredibly useful way to delegate permissions/access to your applications and services without having to store traditional credentials. In this specific case, the IAM role will be assumed by the AWS Lambda service so that your function has the neccessary permissions to carry out whatever actions it needs to. The permissions that you attach to the role will vary depending on the purpose of the function. If you're using the sample function I've provided, then some basic \"Read\" permissions on EC2 resources will be suffice. Go to the IAM console, and create a new role. Choose \"Lambda\" for the service. For the policy, AWS actually provides an existing policy we can use. Choose \"AmazonEC2ReadOnlyAccess\". On the \"Review\" screen, ensure everything looks correct. Give your a function a name that at least alludes to its primary function. I named mine ReadEC2StatusLambda . Be sure to save the role ARN for later, as you'll need it for the IAM user policy. Finally, we need to create an IAM user that Travis-CI can utilize to integrate with AWS. From the IAM console, you will need to navigate to \"Users\", where you view existing users, as well as create a new one. Go ahead and hit the \"Add User\" button. Start by giving the user a meaningful name that will make it easy to identify its purpose, like: TravisServerlessDeploymentUser . This will be a \"service account\", so for \"Access Type\", we're only going to choose \"programmatic access\". For permissions, choose \"Attach existing policies directly\", then hit the \"Create policy\" button. Choose the \"JSON\" tab, and enter the following into the policy editor: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"AllowListArtifactBucket\", \"Effect\": \"Allow\", \"Action\": \"s3:ListBucket\", \"Resource\": \"arn:aws:s3:::<your-bucket-name-here>\" }, { \"Sid\": \"AllowArtifactBucketUse\", \"Effect\": \"Allow\", \"Action\": [ \"s3:PutObjectAcl\", \"s3:PutObject\", \"s3:GetObjectAcl\", \"s3:GetObject\", \"s3:DeleteObject\" ], \"Resource\": \"arn:aws:s3:::<your-bucket-name-here>/*\" }, { \"Sid\": \"AllowLambdaAll\", \"Effect\": \"Allow\", \"Action\": \"lambda:*\", \"Resource\": \"*\" }, { \"Sid\": \"AllowCloudFormationAll\", \"Effect\": \"Allow\", \"Action\": \"cloudformation:*\", \"Resource\": \"*\" }, { \"Sid\": \"AllowIAMListPolicies\", \"Effect\": \"Allow\", \"Action\": \"iam:ListPolicies\", \"Resource\": \"*\" }, { \"Sid\": \"AllowPassLambdaRole\", \"Effect\": \"Allow\", \"Action\": \"iam:PassRole\", \"Resource\": \"<your-role-arn-here>\" } ] } Give the policy a meaningful name, similar to the IAM user you created previously. Be sure to replace both instances of <your-bucket-name-here> with the name of the bucket you created previously. You will also need to replace <your-role-arn-here> with the ARN of the role you generated in the previous step. Review the policy and save it. Afterwards you'll be returned to the user creation process. Hit \"refresh\" on the policy list, and search for the policy you just created. Select the policy, and proceed to \"Review\". If everything looks good, click \"Create User\". On the next screen, you'll be shown the access key and secret key. Make sure you save these in a safe place: this will be the only time you are shown them, and we'll need them for Travis-CI integration. Now we can move on to configuring our Travis-CI integration.","title":"Deploy Your AWS Resources"},{"location":"posts/deploy-aws-lambda-with-travisci-2/#configure-travis-ci-integration","text":"In Part 1 of this series, some basic prerequisites were laid out, including an account on both GitHub and Travis-CI. Signing in to Travis-CI with your GitHub account, via OAuth, should make all your repositories available for Travis to integrate with. Check the Getting Started documentation if you need help. Once you get logged in to Travis-CI, you'll need to enable it for the specific respository you're trying to integrate with. Under your \"Profile\", it will list all your public repositories. If you want to integrate with private repositories, it requires a paid subscription on their .com site. Once that's complete, you now have working Travis-CI integration! The central component required for Travis-CI is the config file: .travis-ci.yml . This is a YAML dotfile that provides a variety of configuration options, which tell Travis \"how\" to build your software. Rather than provide an exhaustive dive on every possible option(of which there are many ), I'll provide the basic file I used for this project, and explain the relevant bits. I highly suggest taking the time to go through the official Travis-CI documentation to familiarize yourself with the options available, as well as the high-level concepts of how it operates. My configuration file is as follows: language: python python: - '2.7' branches: only: master install: - pip install awscli - pip install aws-sam-cli script: - sam validate - sam package --template-file template.yaml --s3-bucket <your-bucket-name> --output-template-file packaged.yaml deploy: provider: script script: sam deploy --template-file packaged.yaml --stack-name travis-serverless-test skip_cleanup: true on: branch: master notifications: email: on_failure: always env: global: - AWS_DEFAULT_REGION=us-east-2 - secure: <obfuscated AWS Access Key> - secure: <obfuscated AWS Secret Key> Let's take a walk through the configuration: language: python The language setting specifies the build environment that Travis uses for the project. Since we need to install awscli and aws-sam-cli via pip, Python is the natural choice. python: - '2.7' Defines what version of Python the project uses. In this case, due to a dependency with aws-sam-cli , we need Python 2. branches: only: master This setting ensures that builds are only triggered on commits to the \"master\" branch of the repo. install: - pip install awscli - pip install aws-sam-cli The \"install\" is the step where we install the packages/dependencies needed for our build. In this example, we use the Pip tool to install awscli and aws-sam-cli . script: - sam validate - sam package --template-file template.yaml --s3-bucket <your-bucket-name> --output-template-file packaged.yaml The \"script\" block is where the default build steps can be overridden. In this case, we call the sam validate command from the aws-sam-cli package to validate our SAM template, then sam package to build our function into a deployable artifact. Note that <your-bucket-name> should be replaced with the name of the S3 bucket that was created earlier in the tutorial. This is where the packaged deployment artifact is stored. Also note the --output-template-file packaged.yaml . This is a YAML file that will be utilized by the deployment step. deploy: provider: script script: sam deploy --template-file packaged.yaml --stack-name travis-serverless-test skip_cleanup: true on: branch: master The \"deploy\" step is where we actually deploy our code to whatever provider we've defined. Travis-CI supports many different providers directly: https://docs.travis-ci.com/user/deployment/#Supported-Providers . In this case we're using the generic script provider, which passes whatever build artifacts we generated in our build step to custom scripts or commands. Our command is sam deploy , which references the packaged.yaml artifact from the build step, as well as defines the name of the \"stack\". I've chosen travis-serverless-test , but feel free to employ whatever naming scheme you choose. This will actually be the name of a CloudFormation stack that the sam tool creates to deploy the Lambda function. skip_cleanup: true prevents Travis-CI from deleting any changes made during the build. on: branch: master ensures that deployments only occur on changes to the master branch. A possible configuration would be to allow builds on all branches, but deploys only on \"master\". notifications: email: on_failure: always Notifications defines how you are notified when things occur during the course of a build/deployment. In this example I've configured it notify via email whenever there is a failure. env: global: - AWS_DEFAULT_REGION=<your-aws-region> - secure: <obfuscated AWS Access Key> - secure: <obfuscated AWS Secret Key> The \"env\" block is where environment variables are defined. These are variables that can be made available for any stage of the process. I highly recommend reading the official docs: https://docs.travis-ci.com/user/environment-variables/ . Environment variables are a critical component of most CI systems, and familiarity with them will be a benefit going forward. For this particular use case, we need 3 specific environment variables defined: AWS_DEFAULT_REGION - Defines which region we are deploying resources to, and is required by the aws-sam-cli tool. I use us-east-2 . You'll want to choose the nearest region that has the Lambda and CloudFormation services available. secure - These are secure(encrypted) variables. In this particular case, we're using them to store the credentials of the IAM user we created previously. Again, the Travis-CI documentation: https://docs.travis-ci.com/user/environment-variables/#Defining-encrypted-variables-in-.travis.yml provides a great resource to explain how. In this case, I used the travis gem to add encrypted variables to my .travis-ci.yml file. If you're worried about sensitive credentials being stored in your shell history, most shells will not write a command to the hist file if you prepend them with a space. You can also add the variables via the Travis-CI site, where they will be obfuscated. However they will show up in the deployment log. I highly recommend using the gem method described above. Now Travis-CI is configured, and we're ready to deploy our code.","title":"Configure Travis-CI Integration"},{"location":"posts/deploy-aws-lambda-with-travisci-2/#deploy-your-function","text":"Your project directory should look fairly similar to this(not counting .git* files): get-active-ec2-instances |-- .travis.yml |-- README.md |-- event.json |-- getinstances | |-- __init__.py | `-- getinstances.py `-- template.yaml If it looks slightly different, that's ok. As long as you have, at a bare minimum, a template.yaml and a .travis-ci.yml file, as well as your function, you should be good to go. At this point, if you commit and push your code to GitHub, it should trigger a job in Travis-CI. If you navigate to the Travis-CI site, the UI should show your repository and job status. Pay attention to the \"Job Log\", as that will show any errors or relevant output from the job. If everything went well: Congratulations! You just did a CI deployment of a serverless function! If you get any errors, check the output of the log for clues as to the root cause. Travis-CI provides some documentation on common failures as well . The last step we'll do is verify our Lambda function deployed correctly, via the AWS console.","title":"Deploy Your Function"},{"location":"posts/deploy-aws-lambda-with-travisci-2/#verify-it-works","text":"Now it's time to make sure everything worked as it should have. Log in to your AWS account console, and navigate to the \"Lambda\" service under \"Compute\". Make sure you're in the correct region(top right). You should see your function. If the name has additional alphanumeric characters appended to it, that's perfectly normal. The sam tool appends a unique ID to every function/stack it deploys. Click on your function. At the top of the interface, you'll see a \"Test\" button next to a drop-down menu that says Select a test event.. Test events provide simulated JSON payloads from various events in AWS that are used to trigger Lambda functions. Go ahead and choose \"Scheduled Event\" from the \"Event Template\" drop down. Name your event something meaningful like \"TestServerlessEvent\". Once you've saved the event, click \"Test\". Depending on your function, you'll see the output generated in the logs, or sent somewhere. In the case of the example provided in the tutorial, it should show a JSON block listing active EC2 instances.","title":"Verify It Works"},{"location":"posts/deploy-aws-lambda-with-travisci-2/#wrap-up","text":"I'm hopeful this article series has provided at least a foundation for everyone to start thinking about ways to streamline serverless development and deployment. Some potential ideas to build on this project: Use the built-in Travis-CI Lambda provider. Add an API gateway to the project. Create a more complex commit/merge workflow for builds. Add unit-testing and code-coverage to builds. Add integration and functional testing to verify deployments. Comments? Questions? Issues? Please post in the comments. Until next time!","title":"Wrap Up"},{"location":"posts/devsecops-add-docker-image-scanning-to-your-cicd-pipeline/","text":"It's been over a year since I've last posted anything to the blog that wasn't commissioned writing. Maintaining consistency in writing definitely falls under a \"hard\" thing for me. I think to get 2022 started right, I'll start with a short technical how-to. I've found others to be immensely helpful, so I enjoy any opportunity to contribute back to the same ecosystem. This post will cover using trivy and GithHub Actions to add security scanning to Docker-based development workflows. Prerequisites A GitHub account Comfortable with Git and Docker workflows. Familiarity with GitHub Actions and basic CI/CD concepts Basic knowledge of Flask/Python(feel free to substitute a different stack) An application that runs inside a Docker container(I'll be using a basic Flask app as an example) The \"Why\": DevSecOps Anyone that's lived through it will tell you that it's not fun to find a security vulnerability in a live production SLA environment. Customer data is potentially at risk, and the future of the company may be in doubt. DevSecOps is a growing culture within DevOps that aims to shift more of the responsibility and implementation duty of security \"left\" in the software development lifecycle(SDLC). Imagine the life of an application, starting with design and the first lines of code written on the left of the timeline, and the final, live application on the right. The further left that security issues can be detected and remediated means fixes are easier to implement and less costly. Developers working with Docker-based applications will often grab base images from public repositories without much thought as to the source. Unfortunately there have been multiple examples of public images being compromised by malicious actors . The good news is that DevOps teams can implement tools to detect and prevent compromise without having expend much effort on implementation. Trivy , from Aquasec security, provides an open source tool that already has a GitHub Action integration available . Getting Started: The App For simplicity's sake, I'll use the basic \"Hello World\" Flask app from the Flask quickstart . My basic Dockerfile is as follows: FROM python:3.9 COPY requirements.txt requirements.txt RUN pip install -r requirements.txt COPY app.py app.py ENV FLASK_APP=\"hello\" CMD [\"python\", \"app.py\"] GitHub Actions Setup With GitHub Actions, we have access to a fully-featured CI/CD toolchain without needing to setup, deploy, and administer our own CI/CD infrastructure. I highly recommend checking out their docs if you're not familiar. Similar to other solutions, GitHub Actions uses YAML files to configure CI/CD pipelines. For this example, we'll use .github/workflows/main.yml . Start by creating a basic definition file that will allow the Docker container to be built in the pipeline: name: build on: push: branches: - main pull_request: branches: - main jobs: build: name: Build runs-on: ubuntu-latest steps: - name: Checkout code uses: actions/checkout@v2 - name: Build the image run: docker build -t codevbus/flask-app -f Dockerfile . Once this file is pushed to the repository, any subsequent push or pull request to the main branch will trigger this workflow, which will build the Docker container based on the Dockerfile in the base directory. Adding image scanning with Trivy The simple syntax of GitHub Actions configuration makes it easy to add additional steps to our workflow file. In this case, we'll be adding the Trivy vulnerability scanner: name: build on: push: branches: - main pull_request: branches: - main jobs: build: name: Build runs-on: ubuntu-latest steps: - name: Checkout code uses: actions/checkout@v2 - name: Build the image run: docker build -t codevbus/flask-app -f Dockerfile . - name: Run Trivy vulnerability scanner uses: aquasecurity/trivy-action@master with: image-ref: codevbus/flask-app format: 'table' exit-code: '1' ignore-unfixed: true vuln-type: 'os,library' severity: 'CRITICAL,HIGH' Once this change is pushed to the repo, the next run of GitHub Actions will use the Trivy action to perform a vulnerability scan of your Docker image and application dependencies. Since I'm using a fairly up-to-date Python version and Docker container, and a very simple app, Trivy shows a clean bill of health: codevbus/flask-app (debian 11.2) ================================ Total: 0 (HIGH: 0, CRITICAL: 0) Python (python-pkg) =================== Total: 0 (HIGH: 0, CRITICAL: 0) You can view this output by going to \"Actions\", clicking through the most recent workflow run, and expanding the log output for the \"Run Trivy vulnerability scanner\" step(as we defined in the main.yml file) If Trivy detected a vulnerability or issue of some kind, this line in the config: exit-code: '1' ensures that a non-zero exit code is thrown and the action/workflow run will not succeed, throwing an obvious warning for further investigation. Wrap-up All done! You should now have a working CI/CD Docker-based development workflow with vulnerability scanning. The example I've provided is overly simple, but additional features could be added, such as commenting scan results to pull requests, or adding static code scanning. All example code and config is provided here: https://github.com/codevbus/flask-app Hopefully this has been helpful, happy hacking!","title":"DevSecOps: Add Docker Image Scanning To Your CI/CD Pipeline"},{"location":"posts/devsecops-add-docker-image-scanning-to-your-cicd-pipeline/#prerequisites","text":"A GitHub account Comfortable with Git and Docker workflows. Familiarity with GitHub Actions and basic CI/CD concepts Basic knowledge of Flask/Python(feel free to substitute a different stack) An application that runs inside a Docker container(I'll be using a basic Flask app as an example)","title":"Prerequisites"},{"location":"posts/devsecops-add-docker-image-scanning-to-your-cicd-pipeline/#the-why-devsecops","text":"Anyone that's lived through it will tell you that it's not fun to find a security vulnerability in a live production SLA environment. Customer data is potentially at risk, and the future of the company may be in doubt. DevSecOps is a growing culture within DevOps that aims to shift more of the responsibility and implementation duty of security \"left\" in the software development lifecycle(SDLC). Imagine the life of an application, starting with design and the first lines of code written on the left of the timeline, and the final, live application on the right. The further left that security issues can be detected and remediated means fixes are easier to implement and less costly. Developers working with Docker-based applications will often grab base images from public repositories without much thought as to the source. Unfortunately there have been multiple examples of public images being compromised by malicious actors . The good news is that DevOps teams can implement tools to detect and prevent compromise without having expend much effort on implementation. Trivy , from Aquasec security, provides an open source tool that already has a GitHub Action integration available .","title":"The \"Why\": DevSecOps"},{"location":"posts/devsecops-add-docker-image-scanning-to-your-cicd-pipeline/#getting-started-the-app","text":"For simplicity's sake, I'll use the basic \"Hello World\" Flask app from the Flask quickstart . My basic Dockerfile is as follows: FROM python:3.9 COPY requirements.txt requirements.txt RUN pip install -r requirements.txt COPY app.py app.py ENV FLASK_APP=\"hello\" CMD [\"python\", \"app.py\"]","title":"Getting Started: The App"},{"location":"posts/devsecops-add-docker-image-scanning-to-your-cicd-pipeline/#github-actions-setup","text":"With GitHub Actions, we have access to a fully-featured CI/CD toolchain without needing to setup, deploy, and administer our own CI/CD infrastructure. I highly recommend checking out their docs if you're not familiar. Similar to other solutions, GitHub Actions uses YAML files to configure CI/CD pipelines. For this example, we'll use .github/workflows/main.yml . Start by creating a basic definition file that will allow the Docker container to be built in the pipeline: name: build on: push: branches: - main pull_request: branches: - main jobs: build: name: Build runs-on: ubuntu-latest steps: - name: Checkout code uses: actions/checkout@v2 - name: Build the image run: docker build -t codevbus/flask-app -f Dockerfile . Once this file is pushed to the repository, any subsequent push or pull request to the main branch will trigger this workflow, which will build the Docker container based on the Dockerfile in the base directory.","title":"GitHub Actions Setup"},{"location":"posts/devsecops-add-docker-image-scanning-to-your-cicd-pipeline/#adding-image-scanning-with-trivy","text":"The simple syntax of GitHub Actions configuration makes it easy to add additional steps to our workflow file. In this case, we'll be adding the Trivy vulnerability scanner: name: build on: push: branches: - main pull_request: branches: - main jobs: build: name: Build runs-on: ubuntu-latest steps: - name: Checkout code uses: actions/checkout@v2 - name: Build the image run: docker build -t codevbus/flask-app -f Dockerfile . - name: Run Trivy vulnerability scanner uses: aquasecurity/trivy-action@master with: image-ref: codevbus/flask-app format: 'table' exit-code: '1' ignore-unfixed: true vuln-type: 'os,library' severity: 'CRITICAL,HIGH' Once this change is pushed to the repo, the next run of GitHub Actions will use the Trivy action to perform a vulnerability scan of your Docker image and application dependencies. Since I'm using a fairly up-to-date Python version and Docker container, and a very simple app, Trivy shows a clean bill of health: codevbus/flask-app (debian 11.2) ================================ Total: 0 (HIGH: 0, CRITICAL: 0) Python (python-pkg) =================== Total: 0 (HIGH: 0, CRITICAL: 0) You can view this output by going to \"Actions\", clicking through the most recent workflow run, and expanding the log output for the \"Run Trivy vulnerability scanner\" step(as we defined in the main.yml file) If Trivy detected a vulnerability or issue of some kind, this line in the config: exit-code: '1' ensures that a non-zero exit code is thrown and the action/workflow run will not succeed, throwing an obvious warning for further investigation.","title":"Adding image scanning with Trivy"},{"location":"posts/devsecops-add-docker-image-scanning-to-your-cicd-pipeline/#wrap-up","text":"All done! You should now have a working CI/CD Docker-based development workflow with vulnerability scanning. The example I've provided is overly simple, but additional features could be added, such as commenting scan results to pull requests, or adding static code scanning. All example code and config is provided here: https://github.com/codevbus/flask-app Hopefully this has been helpful, happy hacking!","title":"Wrap-up"},{"location":"posts/happy-fourth-2020/","text":"No new content this week as I enjoy a long holiday weekend. Hopefully everyone is able to take some enjoyment and happiness from this weekend, even in these unprecedented times. Stay safe and healthy!","title":"Happy Fourth 2020"},{"location":"posts/how-i-auto-publish-posts/","text":"My ongoing focus has been an emphasis on reducing as much friction as I can in writing and publishing. The consideration of ideas and knowledge, and translating them to my own words and mental models, still has a lot of value. Conversely, the manual and repetitive effort of publishing, of saving files and making git pushes, does not for me. There may be windows of time in which I generate a lot of content. Ostensibly, a theme of popular online publishing outlets is that there is a consistent delivery of quality content, rather than an inconsistent and bursty model. How to reconcile inconsisent content generation with consistent content publishing ? Hugo , the software this blog is built with, allows you to define a future date of publication. However, for this to be triggered, the \"engine\" of the blog still has to rebuild the content. Netlify provides a modern, CDN-esque platform for hosting static websites and blogs. One of their features includes providing a build \"hook\", which is a unique URL, that when visited, will trigger a rebuild of the site. When combined with Github Actions , I can have my site automatically rebuild itself every midnight, automatically publishing any post-dated content. Prerequisites If you want to duplicate this functionality, you will need: A static site generator. I use Hugo. Other options include Jekyll, Gatsby, and Nuxt. Site content in version control like Github or Gitlab. Netlify hosting for your static site. If not Github actions, some other kind of cron-emulation service. Generate A Build Hook The first thing I did was generate a unique build hook in my \"Site Settings\" in the Netlify admin UI: ...went to \"Build & deploy\": ...and finally added a \"Build hook\": Once added, the build hook will be a Netlify API URL, with a unique alphanumeric identifier. Calls to this URL will trigger a site rebuild on Netlify. Now I can input this URL into Github Actions. Automatically Trigger The Build Hook With Github Actions I originally set this up using the UI, as I wanted to explore it a bit, but it's a lot more efficient to create and commit a file from the IDE. First, I would create the directory structure needed for Github to interpret a valid action config(this command would be done in the root directory of the blog): $ mkdir -p .github/workflows Then, I create a file in the \"workflows\" directory named \"main.yml\": $ touch .github/workflows/main.yml The contents are as follows: name: auto-build on: schedule: - cron: '0 5 * * *' jobs: build: runs-on: ubuntu-latest steps: - name: Trigger Netlify build run: curl -X POST -d '{}' https://api.netlify.com/build_hooks/${{ secrets.NETLIFY_HOOK }} With this workflow definition, the Netlify build hook will be triggered daily, at EST midnight. I want to avoid exposing my build hook id publicly, as that means anyone could trigger a build of my site, and possibly get my Netlify API access throttled or locked. For the last step, I'll set up a Github secret that can be interpolated in my config. Define a Github Secret First, I go to repo \"Settings\": ...then \"Secrets\": I clicked \"Add a new secret\", named it \"NETLIFY_HOOK\", and populated the value with the URL id. Now, whenever I want to have that secret available in a config, without revealing it, I can interpolate it with the following syntax: ${{ secrets.NETLIFY_HOOK }} Summary With minimal config and effort, I've got automation to rebuild every midnight, at which point any approriately dated post will then be published.","title":"how I auto publish posts"},{"location":"posts/how-i-auto-publish-posts/#prerequisites","text":"If you want to duplicate this functionality, you will need: A static site generator. I use Hugo. Other options include Jekyll, Gatsby, and Nuxt. Site content in version control like Github or Gitlab. Netlify hosting for your static site. If not Github actions, some other kind of cron-emulation service.","title":"Prerequisites"},{"location":"posts/how-i-auto-publish-posts/#generate-a-build-hook","text":"The first thing I did was generate a unique build hook in my \"Site Settings\" in the Netlify admin UI: ...went to \"Build & deploy\": ...and finally added a \"Build hook\": Once added, the build hook will be a Netlify API URL, with a unique alphanumeric identifier. Calls to this URL will trigger a site rebuild on Netlify. Now I can input this URL into Github Actions.","title":"Generate A Build Hook"},{"location":"posts/how-i-auto-publish-posts/#automatically-trigger-the-build-hook-with-github-actions","text":"I originally set this up using the UI, as I wanted to explore it a bit, but it's a lot more efficient to create and commit a file from the IDE. First, I would create the directory structure needed for Github to interpret a valid action config(this command would be done in the root directory of the blog): $ mkdir -p .github/workflows Then, I create a file in the \"workflows\" directory named \"main.yml\": $ touch .github/workflows/main.yml The contents are as follows: name: auto-build on: schedule: - cron: '0 5 * * *' jobs: build: runs-on: ubuntu-latest steps: - name: Trigger Netlify build run: curl -X POST -d '{}' https://api.netlify.com/build_hooks/${{ secrets.NETLIFY_HOOK }} With this workflow definition, the Netlify build hook will be triggered daily, at EST midnight. I want to avoid exposing my build hook id publicly, as that means anyone could trigger a build of my site, and possibly get my Netlify API access throttled or locked. For the last step, I'll set up a Github secret that can be interpolated in my config.","title":"Automatically Trigger The Build Hook With Github Actions"},{"location":"posts/how-i-auto-publish-posts/#define-a-github-secret","text":"First, I go to repo \"Settings\": ...then \"Secrets\": I clicked \"Add a new secret\", named it \"NETLIFY_HOOK\", and populated the value with the URL id. Now, whenever I want to have that secret available in a config, without revealing it, I can interpolate it with the following syntax: ${{ secrets.NETLIFY_HOOK }}","title":"Define a Github Secret"},{"location":"posts/how-i-auto-publish-posts/#summary","text":"With minimal config and effort, I've got automation to rebuild every midnight, at which point any approriately dated post will then be published.","title":"Summary"},{"location":"posts/in-defense-of-devops/","text":"Lately there seems to be a growing chorus within software engineering that DevOps is \"dead\", or at least an abject failure. I'd argue it is more important than ever before. Software delivery is an increasingly complex beast, fraught with security threats and requiring a pragmatic and nuanced approach to operational excellence. Engineering organizations cannot afford to maintain development teams with the attitude that they should not have to care at all about anything outside the scope of writing code. When Did We Get So Precious About Development Work? Devs saying that they shouldn't have to do ops or have any concern for the underlying infrastructure sounds like a familiar refrain... where have we all heard that before? Look familiar? It should. Here's a hint: It's from the original Allspaw/Hammond presentation on DevOps, way back in 2009. \"Traditional thinking\" was the anti-pattern; the old way of doing things that had Dev and Ops in separate silos, with devs only concerned about \"shipping code\" and ops staff making it run. If we're back to devs not needing to care about infrastructure and operational concerns, then it really feels like we've made a full-circle return to the waterfall days of development. Bridge builders do not get to claim ignorance as to the nature of cars and trucks, or the lead time required to make safe structural steel, and yet here we are. DevOps will be opinionated and force things on developers, because, well, some things aren't really optional... DevOps Often Becomes The Proxy Enforcer of Security Policy... If you think the ratio of DevOps engineers to developers in your company is thin, have you looked at the ratio of security engineers to all other employees? Yeah... Thus, it is often left to DevOps teams building out workload and deployment systems to also be the proxy enforcer of security policy via these very systems. Security policies by nature tend to be very opinionated, and are generally viewed as being \"in the way\" of feature iteration. They also happen to be in the way for a very good reason: https://blog.gitguardian.com/the-state-of-secrets-sprawl-2022/ From their report: The data reveals that on average, in 2021, a typical company with 400 developers would discover 1,050 unique secrets leaked upon scanning its repositories and commits. With each secret detected in 13 different places on average, the amount of work required for remediation far exceeds current AppSec capabilities: with a security-to-developers ratio of 1:100, 1 AppSec engineer needs to handle 3,413 secrets occurrences on average. Yikes. No, we cannot make developers learn, or care about, something like Terraform, or whatever other ops tooling or process we try to convince everyone is needed. What we shouldn't have to do is justify the use of tools or processes that takes us to a more secure, more operationally excellent posture; even if it means some perceived inconvenience to developer productivity. I absolutely concur that the idea of a platform team ends up being too opinionated; the golden path turns into a fixed point-in-time lock-in that will never keep up with the pace of iteration by the parent platform provider. However, enforcing standards and secure software delivery is probably an opinion that would benefit the overall health(and continued existence of the business). Even something banal as getting resources named and tagged correctly can be important because... ... and the Torch-Bearer for Audit Compliance as well Anyone that's ever been beholden to a compliance framework or regular audits knows that it is a time-consuming, nerve-wracking, stressful process. Auditors often ask for(as they should) very detailed, granular questions about specific processes and systems. These questions typically demand some form of paper trail(typically in the form of logs) that can establish time, date, and metadata for a particular event. A hypothetical audit request might read like: \"According to the logs of this critical_datastore, Developer A accessed this piece of data. Please provide a timestamped log of your application control plane, showing that the developer followed the correct access pattern and used 2FA for authentication\" Sure. No problem. Except there are 25 nodes that classify as \"control planes\". Why are some named with this scheme: \"controlPlane-01\", others named with: \"ControlP2\", and one just named \"-07\"??? Ugh. Fine, no problem. I'll just look at the log aggregator where all these stream to. Ok no log aggregation, of course that was stupid to assume. Welp. I guess I'll just have to look at each node's logs individually. Ahahahahahahahahahaha why did I dare let hope's light illuminate the darkness only for it to be extinguished? Enforcing even the boring stuff like naming pays dividends later when it comes time to hopefully present auditors with a picture that your organization is at least making a good-faith effort to comply with the rules. So What's The Answer? I think DevOps teams should absolutely be out of the way when it comes to platform and implementation choice. If the product team A wants to use serverless infrastructure, product team B wants containers, and product team C just needs an S3 bucket to host pictures, so be it. DevOps should be able to provide modular, primitive infrastructure patterns that allow developers to take advantage of the infrastructure that best suits their needs. However.. This cannot and should not imply that it's Wild West time. If product A's choice of serverless means there are extra development cycles needed to implement proper security controls and sane operational patterns then that's the cost of business continuity and developers are going to have to care about it whether it's part of their KPIs or not. It takes time to build even the most unobtrusive of guardrails in such a way that they are both unobtrusive and actually effective at their stated purpose. These folks also might know a thing or two about the problem space: https://itrevolution.com/accelerate-book/ ... \"We speculate that this is due to low-performing teams working to increase tempo but not investing enough in building quality into the process. The result is larger deployment failures that take more time to restore service. High performers understand that they don\u2019t have to trade speed for stability or vice versa, because by building quality in they get both.\"","title":"In Defense of DevOps"},{"location":"posts/in-defense-of-devops/#when-did-we-get-so-precious-about-development-work","text":"Devs saying that they shouldn't have to do ops or have any concern for the underlying infrastructure sounds like a familiar refrain... where have we all heard that before? Look familiar? It should. Here's a hint: It's from the original Allspaw/Hammond presentation on DevOps, way back in 2009. \"Traditional thinking\" was the anti-pattern; the old way of doing things that had Dev and Ops in separate silos, with devs only concerned about \"shipping code\" and ops staff making it run. If we're back to devs not needing to care about infrastructure and operational concerns, then it really feels like we've made a full-circle return to the waterfall days of development. Bridge builders do not get to claim ignorance as to the nature of cars and trucks, or the lead time required to make safe structural steel, and yet here we are. DevOps will be opinionated and force things on developers, because, well, some things aren't really optional...","title":"When Did We Get So Precious About Development Work?"},{"location":"posts/in-defense-of-devops/#devops-often-becomes-the-proxy-enforcer-of-security-policy","text":"If you think the ratio of DevOps engineers to developers in your company is thin, have you looked at the ratio of security engineers to all other employees? Yeah... Thus, it is often left to DevOps teams building out workload and deployment systems to also be the proxy enforcer of security policy via these very systems. Security policies by nature tend to be very opinionated, and are generally viewed as being \"in the way\" of feature iteration. They also happen to be in the way for a very good reason: https://blog.gitguardian.com/the-state-of-secrets-sprawl-2022/ From their report: The data reveals that on average, in 2021, a typical company with 400 developers would discover 1,050 unique secrets leaked upon scanning its repositories and commits. With each secret detected in 13 different places on average, the amount of work required for remediation far exceeds current AppSec capabilities: with a security-to-developers ratio of 1:100, 1 AppSec engineer needs to handle 3,413 secrets occurrences on average. Yikes. No, we cannot make developers learn, or care about, something like Terraform, or whatever other ops tooling or process we try to convince everyone is needed. What we shouldn't have to do is justify the use of tools or processes that takes us to a more secure, more operationally excellent posture; even if it means some perceived inconvenience to developer productivity. I absolutely concur that the idea of a platform team ends up being too opinionated; the golden path turns into a fixed point-in-time lock-in that will never keep up with the pace of iteration by the parent platform provider. However, enforcing standards and secure software delivery is probably an opinion that would benefit the overall health(and continued existence of the business). Even something banal as getting resources named and tagged correctly can be important because...","title":"DevOps Often Becomes The Proxy Enforcer of Security Policy..."},{"location":"posts/in-defense-of-devops/#and-the-torch-bearer-for-audit-compliance-as-well","text":"Anyone that's ever been beholden to a compliance framework or regular audits knows that it is a time-consuming, nerve-wracking, stressful process. Auditors often ask for(as they should) very detailed, granular questions about specific processes and systems. These questions typically demand some form of paper trail(typically in the form of logs) that can establish time, date, and metadata for a particular event. A hypothetical audit request might read like: \"According to the logs of this critical_datastore, Developer A accessed this piece of data. Please provide a timestamped log of your application control plane, showing that the developer followed the correct access pattern and used 2FA for authentication\" Sure. No problem. Except there are 25 nodes that classify as \"control planes\". Why are some named with this scheme: \"controlPlane-01\", others named with: \"ControlP2\", and one just named \"-07\"??? Ugh. Fine, no problem. I'll just look at the log aggregator where all these stream to. Ok no log aggregation, of course that was stupid to assume. Welp. I guess I'll just have to look at each node's logs individually. Ahahahahahahahahahaha why did I dare let hope's light illuminate the darkness only for it to be extinguished? Enforcing even the boring stuff like naming pays dividends later when it comes time to hopefully present auditors with a picture that your organization is at least making a good-faith effort to comply with the rules.","title":"... and the Torch-Bearer for Audit Compliance as well"},{"location":"posts/in-defense-of-devops/#so-whats-the-answer","text":"I think DevOps teams should absolutely be out of the way when it comes to platform and implementation choice. If the product team A wants to use serverless infrastructure, product team B wants containers, and product team C just needs an S3 bucket to host pictures, so be it. DevOps should be able to provide modular, primitive infrastructure patterns that allow developers to take advantage of the infrastructure that best suits their needs. However.. This cannot and should not imply that it's Wild West time. If product A's choice of serverless means there are extra development cycles needed to implement proper security controls and sane operational patterns then that's the cost of business continuity and developers are going to have to care about it whether it's part of their KPIs or not. It takes time to build even the most unobtrusive of guardrails in such a way that they are both unobtrusive and actually effective at their stated purpose. These folks also might know a thing or two about the problem space: https://itrevolution.com/accelerate-book/ ... \"We speculate that this is due to low-performing teams working to increase tempo but not investing enough in building quality into the process. The result is larger deployment failures that take more time to restore service. High performers understand that they don\u2019t have to trade speed for stability or vice versa, because by building quality in they get both.\"","title":"So What's The Answer?"},{"location":"posts/its-been-awhile/","text":"Feels like I've arrived at the inevitable trough of blogging activity: months between updates, with \"projects\" occupying the majority of my time. Once upon a time, posting about the content I was reading/consuming forced me to keep a better posting discipline. However, that started to feel like it was more of a signaling exercise than something of actual cognitive value. I'm still sitting on my no-code project site, but the rush of content/ideas/people into the space has caused me to pause. I want to make sure it adds value, without turning into an excessive amount of work(or it may end up in the bin of unfinished projects, as so many often do). Aside from my day job, opportunities for freelance technical content writing have started to really pick up. There's heavy demand for well-written, engaging technical content, so I've been taking advantage. Getting more opportunities to flex the writing muscle means it will only get stronger. Unfortunately that might also make writing for myself more of a chore. For the upcoming year, I'm going to try and keep some loosely defined goals, and see if I can keep to them. Goals for 2021 In no particular order: * Balance paid writing with blogging(aiming for 1 post a week cadence) * Launch at least 3 sites/projects * Get at least 100 Twitter followers * Finish some of the ever-growing pile of technical books I seem to have Let's see how it goes. Hopefully everyone stays health and safe in the coming year.","title":"It's Been Awhile"},{"location":"posts/its-been-awhile/#goals-for-2021","text":"In no particular order: * Balance paid writing with blogging(aiming for 1 post a week cadence) * Launch at least 3 sites/projects * Get at least 100 Twitter followers * Finish some of the ever-growing pile of technical books I seem to have Let's see how it goes. Hopefully everyone stays health and safe in the coming year.","title":"Goals for 2021"},{"location":"posts/juneteenth/","text":"Rather than resuming my usual cadence of content posts, I thought it would be better to follow up on my last post with a similar theme. Today marks the anniversary of the \"end\" of slavery. While it's a positive direction that our country generally decided it was no longer appropriate to \"own\" another human being, the legacy of racism obviously lives on today. Please utilize the links below to donate Donate to BLM: https://secure.actblue.com/donate/ms%5Fblm%5Fhomepage%5F2019 Donate to the NAACP: https://secure.actblue.com/donate/naacp-1 Donate to the ACLU: https://action.aclu.org/give/now","title":"juneteenth"},{"location":"posts/new-no-code-tutorial/","text":"Following up on my previous post: https://mikevanbuskirk.io/posts/new-site-launch/ , I've written another no code tutorial on Dev.to to further promote the future launch of: nocodeautomation.io : https://dev.to/codevbus/graph-your-dev-to-metrics-with-no-code-2a5j This tutorial goes a little more in depth, showing how to do some simple time-series graphing of Dev.to post metrics using Airtable and Plotly. As always, if you find this content interesting, please sign up on the nocodeautomation.io site for future updates ahead of the site launch. Hope everyone is healthy and safe!","title":"New No Code Tutorial Posted"},{"location":"posts/new-site-launch/","text":"Just a brief post to share something I've been working on/thinking about the past few weeks. I've been experimenting with \"No Code\" tools recently, and I'm continually blown away by the flexibility and power they provide. I'm launching a site dedicated to \"No Code\" tutorials, tips, tools, and news: https://nocodeautomation.io The site isn't live yet, but I'll be sending updates and content to subscribed users. Please sign up if you think this might be useful or interesting, or if you know of someone who may. I posted a tutorial on dev.to to help promote the kind of content that will be featured on the site: https://dev.to/codevbus/track-your-dev-to-metrics-with-no-code-2k5m Look for more updates in the future. I hope everyone is staying healthy and safe!","title":"New Site Launch"},{"location":"posts/pkm-redux/","text":"I think the hallmark of any good note-taking system is that it is never static. It's something you should revisit from time to time. If the workflow no longer makes sense for you, or the tagging system has grown too large, or you simply want a change of pace, it's probably time to refactor and rethink. I've been using org-mode and org-roam for several months now, and I've started to realize that my note-taking and blogging workflow is starting to work against me, rather than for me. My usage pattern is turning out to be more of an \"anti-pattern\". Org mode, and in particular, org-roam, are very powerful tools for note-taking and pkm, I intend to keep using them. That being said, my usage, and metadata, is in dire need of revision. One of the primary goals of the roam/zettelkasten system is that relevant notes(knowledge) are surfaced at exactly the right moment, when they can deliver the greatest value at a certain point in time. Right now, the metadata system I've developed has gotten in the way of this core tenet. Normally, I use some basic automation to publish a list of the content I've consumed for the week. This week that's going on pause while I do some serious pkm house cleaning. My aim is to have a cleaner, more idiomatic tagging and metadata system that will restore my pkm to the idea I had for it when I first started.","title":"pkm redux"},{"location":"posts/something-different/","text":"I think the events of this week merit something a bit more meaningful than a post about technology or entrepreneurial podcasts. There's not a lot I can say, other than to suggest we lift up and help the marginalized voices in our society, who are so desperate to be heard. Please utilize the links below to donate or sign a petition. Donate to BLM: https://secure.actblue.com/donate/ms%5Fblm%5Fhomepage%5F2019 Donate to the NAACP: https://secure.actblue.com/donate/naacp-1 Donate to the ACLU: https://action.aclu.org/give/now Petition for \"Justice For George Floyd\": https://www.change.org/p/mayor-jacob-frey-justice-for-george-floyd","title":"something different"},{"location":"posts/terraform-backend-with-cloudformation/","text":"Defining your infrastructure as code has become an integral part of many successful DevOps workflows. Terraform has become a key tool, which I often employ to define a variety of infrastructure. Today I'm going to show you how to set up the basic backend infrastructure needed to allow multiple engineers to collaborate on the same resources in AWS, using Terraform and CloudFormation. Prerequisites To complete the actions described in this tutorial, you'll need access to an AWS account in which you have permissions to access and provision resources. If you haven't already, go ahead and get the AWS Command Line utility installed: Installation Guide and configured: Configuration Guide . DISCLAIMER : Any AWS resources you provision during the course of this tutorial may result in charges being billed to your account. The author assumes no responsibility, legal or otherwise, for charges incurred. What Are We Doing? Why? Now that you have Terraform installed, you should be able to create, modify, and destroy AWS resources. The problem is, this setup is really only good for local testing and development. When it comes time to start deploying production infrastructure, and multiple engineers need to work against the same resource stacks, there needs to be some way to ensure consistency, and to avoid overlapping changes. Enter remote state locking and backends. Take some time to read the official Hashicorp documentation on backends . It explains what a backend is, what it does, and the benefits of employing one. Today we're going to deploy a remote backend with AWS S3, and state locking via AWS DynamoDB. Getting Started With AWS I've already created the basic CloudFormation template to get the infrastructure created. If you want to take a shortcut to get up and running, simply click the \"Launch Stack\" button below and populate your variables as needed. Parameters: StateBucketName: Description: Name of the S3 bucket to place the Terraform state files in. Type: String LockTableName: Description: Name of the DynamoDB table to store state locks. Type: String AWSTemplateFormatVersion: 2010-09-09 Resources: TerraformStateBucket: Type: 'AWS::S3::Bucket' Properties: AccessControl: Private BucketName: !Ref StateBucketName VersioningConfiguration: Status: Enabled TerraformLockTable: Type: 'AWS::DynamoDB::Table' Properties: AttributeDefinitions: - AttributeName: LockID AttributeType: S KeySchema: - AttributeName: LockID KeyType: HASH ProvisionedThroughput: ReadCapacityUnits: 5 WriteCapacityUnits: 5 TableName: !Ref LockTableName Outputs: StackName: Value: !Ref AWS::StackName If you want to launch the stack manually using the AWS CLI, create a file in your working directory called backend.yaml and copy the above code in to it. The following command will launch the stack. You will need to provide your desired values for the bucket name and DynamoDB table: aws cloudformation create-stack --stack-name terraform-backend --template-body file://backend.yaml --parameters ParameterKey=StateBucketName,ParameterValue=<your_bucket_name> ParameterKey=LockTableName,ParameterValue=<your_lock_table_name> Now we can move on to configuring our Terraform infrastructure to utilize our new remote state backend. Configuring Terraform If you've deployed any infrastructure with Terraform, you are likely familiar with the basic, overall structure of a project. You have the binary installed on your local development system, and a root folder for a given project, containing at least one, often several terraform files. Information about the state of the infrastructure is stored locally on your machine. This works well for quick, iterative development work, but presents obvious problems for collaboration. Using the infrastructure we deployed earlier in the tutorial, we'll configure a basic Terraform project to utilize remote state storage and locking. This will enable the ability to collaborate safely, as well as provide a safety net if the infrastructure state becomes corrupted or lost on the local workstation. provider \"aws\" { region = \"<aws_region>\" } terraform { backend \"s3\" { bucket = \"<bucket_name>\" key = \"<folder/filename.tfstate>\" region = \"<aws_region>\" dynamodb_table = \"<dynamo_table_name>\" encrypt = true } } Populate the bucket, region, and dynamodb_table variables with the bucket name and dynamodb table name you chose earlier, as well as the AWS region you are operating out of. Utilizing The Backend We'll test our new backend by running a plan against a very simplistic AWS resource in a new Terraform project. The backend can be used for multiple Terraform projects/environments, so feel free to configure your other projects as needed. Copy the backend.tf file you created earlier to a new project directory. From inside the directory, run the command: $ terraform init Your output should be similar to the following: $ terraform init Initializing the backend... Successfully configured the backend \"s3\"! Terraform will automatically use this backend unless the backend configuration changes. Initializing provider plugins... - Checking for available provider plugins on https://releases.hashicorp.com... - Downloading plugin for provider \"aws\" (1.6.0)... The following providers do not have any version constraints in configuration, so the latest version was installed. To prevent automatic upgrades to new major versions that may contain breaking changes, it is recommended to add version = \"...\" constraints to the corresponding provider blocks in configuration, with the constraint strings suggested below. * provider.aws: version = \"~> 1.6\" Terraform has been successfully initialized! You may now begin working with Terraform. Try running \"terraform plan\" to see any changes that are required for your infrastructure. All Terraform commands should now work. If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary. You can see in the first steps of the output, Terraform initializes and configures the backend. Note that you are required to have the S3 bucket and DynamoDB table already deployed and configured with the correct permissions before Terraform can utilize it. The backend can be tested simply by running terraform plan against a basic resource config. I used an empty security group to validate: resource \"aws_security_group\" \"main\" { name = \"test\" description = \"test sg\" vpc_id = <your_vpc_id> } Be sure to fill in the correct value for your VPC id. Run: $ terraform plan inside the working directory. The import output to validate will come at the beginning: Acquiring state lock. This may take a few moments... and end: Releasing state lock. This may take a few moments... This lets you know that Terraform is interacting correctly with the remote lock table and state file. Conclusion After completing this tutorial successfully, you should have a Cloudformation-defined remote state and locking infrastructure for Terraform. This is a major step towards utilizing Terraform on a larger scale for your infrastructure deployments!","title":"deploy a terraform remote state backend with cloudformation"},{"location":"posts/terraform-backend-with-cloudformation/#prerequisites","text":"To complete the actions described in this tutorial, you'll need access to an AWS account in which you have permissions to access and provision resources. If you haven't already, go ahead and get the AWS Command Line utility installed: Installation Guide and configured: Configuration Guide . DISCLAIMER : Any AWS resources you provision during the course of this tutorial may result in charges being billed to your account. The author assumes no responsibility, legal or otherwise, for charges incurred.","title":"Prerequisites"},{"location":"posts/terraform-backend-with-cloudformation/#what-are-we-doing-why","text":"Now that you have Terraform installed, you should be able to create, modify, and destroy AWS resources. The problem is, this setup is really only good for local testing and development. When it comes time to start deploying production infrastructure, and multiple engineers need to work against the same resource stacks, there needs to be some way to ensure consistency, and to avoid overlapping changes. Enter remote state locking and backends. Take some time to read the official Hashicorp documentation on backends . It explains what a backend is, what it does, and the benefits of employing one. Today we're going to deploy a remote backend with AWS S3, and state locking via AWS DynamoDB.","title":"What Are We Doing? Why?"},{"location":"posts/terraform-backend-with-cloudformation/#getting-started-with-aws","text":"I've already created the basic CloudFormation template to get the infrastructure created. If you want to take a shortcut to get up and running, simply click the \"Launch Stack\" button below and populate your variables as needed. Parameters: StateBucketName: Description: Name of the S3 bucket to place the Terraform state files in. Type: String LockTableName: Description: Name of the DynamoDB table to store state locks. Type: String AWSTemplateFormatVersion: 2010-09-09 Resources: TerraformStateBucket: Type: 'AWS::S3::Bucket' Properties: AccessControl: Private BucketName: !Ref StateBucketName VersioningConfiguration: Status: Enabled TerraformLockTable: Type: 'AWS::DynamoDB::Table' Properties: AttributeDefinitions: - AttributeName: LockID AttributeType: S KeySchema: - AttributeName: LockID KeyType: HASH ProvisionedThroughput: ReadCapacityUnits: 5 WriteCapacityUnits: 5 TableName: !Ref LockTableName Outputs: StackName: Value: !Ref AWS::StackName If you want to launch the stack manually using the AWS CLI, create a file in your working directory called backend.yaml and copy the above code in to it. The following command will launch the stack. You will need to provide your desired values for the bucket name and DynamoDB table: aws cloudformation create-stack --stack-name terraform-backend --template-body file://backend.yaml --parameters ParameterKey=StateBucketName,ParameterValue=<your_bucket_name> ParameterKey=LockTableName,ParameterValue=<your_lock_table_name> Now we can move on to configuring our Terraform infrastructure to utilize our new remote state backend.","title":"Getting Started With AWS"},{"location":"posts/terraform-backend-with-cloudformation/#configuring-terraform","text":"If you've deployed any infrastructure with Terraform, you are likely familiar with the basic, overall structure of a project. You have the binary installed on your local development system, and a root folder for a given project, containing at least one, often several terraform files. Information about the state of the infrastructure is stored locally on your machine. This works well for quick, iterative development work, but presents obvious problems for collaboration. Using the infrastructure we deployed earlier in the tutorial, we'll configure a basic Terraform project to utilize remote state storage and locking. This will enable the ability to collaborate safely, as well as provide a safety net if the infrastructure state becomes corrupted or lost on the local workstation. provider \"aws\" { region = \"<aws_region>\" } terraform { backend \"s3\" { bucket = \"<bucket_name>\" key = \"<folder/filename.tfstate>\" region = \"<aws_region>\" dynamodb_table = \"<dynamo_table_name>\" encrypt = true } } Populate the bucket, region, and dynamodb_table variables with the bucket name and dynamodb table name you chose earlier, as well as the AWS region you are operating out of.","title":"Configuring Terraform"},{"location":"posts/terraform-backend-with-cloudformation/#utilizing-the-backend","text":"We'll test our new backend by running a plan against a very simplistic AWS resource in a new Terraform project. The backend can be used for multiple Terraform projects/environments, so feel free to configure your other projects as needed. Copy the backend.tf file you created earlier to a new project directory. From inside the directory, run the command: $ terraform init Your output should be similar to the following: $ terraform init Initializing the backend... Successfully configured the backend \"s3\"! Terraform will automatically use this backend unless the backend configuration changes. Initializing provider plugins... - Checking for available provider plugins on https://releases.hashicorp.com... - Downloading plugin for provider \"aws\" (1.6.0)... The following providers do not have any version constraints in configuration, so the latest version was installed. To prevent automatic upgrades to new major versions that may contain breaking changes, it is recommended to add version = \"...\" constraints to the corresponding provider blocks in configuration, with the constraint strings suggested below. * provider.aws: version = \"~> 1.6\" Terraform has been successfully initialized! You may now begin working with Terraform. Try running \"terraform plan\" to see any changes that are required for your infrastructure. All Terraform commands should now work. If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary. You can see in the first steps of the output, Terraform initializes and configures the backend. Note that you are required to have the S3 bucket and DynamoDB table already deployed and configured with the correct permissions before Terraform can utilize it. The backend can be tested simply by running terraform plan against a basic resource config. I used an empty security group to validate: resource \"aws_security_group\" \"main\" { name = \"test\" description = \"test sg\" vpc_id = <your_vpc_id> } Be sure to fill in the correct value for your VPC id. Run: $ terraform plan inside the working directory. The import output to validate will come at the beginning: Acquiring state lock. This may take a few moments... and end: Releasing state lock. This may take a few moments... This lets you know that Terraform is interacting correctly with the remote lock table and state file.","title":"Utilizing The Backend"},{"location":"posts/terraform-backend-with-cloudformation/#conclusion","text":"After completing this tutorial successfully, you should have a Cloudformation-defined remote state and locking infrastructure for Terraform. This is a major step towards utilizing Terraform on a larger scale for your infrastructure deployments!","title":"Conclusion"},{"location":"posts/the-verdict-is-in/","text":"Remote work is dead. Remote work is not dead. Wait, no it's dead again. No wait we're just kidding, it's totally awesome. It seems the techo-literistas have swung back and forth between pronouncing the death of remote work, and singing its praises. Well the verdict is in. Remote work... works. An Inc article published last month: https://www.inc.com/scott-mautz/a-2-year-stanford-study-shows-astonishing-productivity-boost-of-working-from-home.html discusses a study done by a Stanford professor, which shows non-trivial boosts to productivity, and a reduction in employee attrition. It seems remote work has bubbled back to the top of frequent discussion topics in places like Hacker News and IT-related subreddits. As a remote worker(and a big proponent of it), this seemed like a good opportunity to put down some random thoughts. The Cycle of Life and Death If you perform a google search for \"the death of remote work\", you'll be treated to a bevy of articles, spanning the course of multiple years. You're likely to notice something in those search results. A definitive cycle of acceptance and novelty, followed by pervasive FUD. The prevailing pattern seems to be as follows: A unique, limited population of pioneers demonstrate remote work is viable More interest is generated amongst job seekers for remote work Remote work seems to be on the path to widespread, general acceptance Larger employers start to embrace some variety of remote work High profile employer recalls remote work policy Sensationalist articles are published, declaring remote work \"dead\" Rinse and repeat Sadly, almost all those articles leave out an important piece of context: to a one, the high profile employers involved in the recall or total elimination of remote work policies did so in the shadow of a prolonged period of lackluster results and ceding market share to competitors. Most corporate leadership teams are not going to make any sweeping changes to such policies when things are going well. Why fix what's not broken? But when things turn south, a lot of management teams head straight for the \"micromanage employee time\" playbook. How do you know when they're using that playbook? Look for some generic statement about \"driving collaboration and innovation\". If that statement seems incredibly disingenuous, it's because it is. There's a couple bits to unpack here. First, collaboration and innovation are not mutually exclusive to being present in an office. The ubiquitousness of online communication and collaboration tools like Slack and WebEx means collaborating with co-workers is easier than ever. Not only that, now you have a logged, searchable record of any innovative ideas that are discussed. Secondly, just how much were employees, no matter their working location, involved in long-term, company-wide product strategy discussions? Investment discussions? Market strategy? These fall squarely on the shoulders of the c-suite. Brass-tacks: the company is struggling, and this is an easy lever to pull to appease stakeholders and investors. The next time you read remote work's eulogy, just be sure to look at the big picture, and really consider the context in which it's being presented. Remote Work Does Not Make You A Better Communicator Real talk: if you're not already comfortable being a robust and prolific communicator, remote work will not fix that. I see a lot of comments from individuals who define themselves as \"socially awkward\", or uncomfortable with face-to-face interaction, or who believe remote work will suddenly make them better able to work with peers. Nope. You need to very comfortable with frequent interaction, including video calls/conferences. Remote workers need to bias on the side of over communicating. As a remote worker, simple physics dictates that you're going to lose a little bit of visibility and mind share when you don't have a physical presence in the office. You'll need to be compensating for that consistently. If you don't imagine yourself as someone who's comfortable communicating frequently with co-workers of varying levels of seniority and technical acumen, who can be available to lead, take ownership, schedule and run meetings, you're going to suffer as a remote worker. Invest In The Tools I've spent close to $2500 on furniture and tools for my office. That may seem like a lot, but that's a drop in the bucket compared to the potential lifetime of medical bills from an RSI. You know what makes it easier to be at your desk for 8 to 10 hours a day, available, motivated, and energetic? A comfortable work environment. A 50, or even 100 or 200 dollar chair from Staples is not going to cut it. The antique wooden desk you pulled out of the attic? Made before ergonomics was really a thing. You can't legitmately pronounce remote work a failure, or not possible, if you don't invest up-front in the kinds of tools that ensure your comfort and health. A Jarvis Bamboo adjustable standing desk and a Herman Miller Aeron chair allow me to stay at my desk for hours with nary pain or stiffness. Do yourself a favor. If remote work is really a long-term commitment, commit to making sure it's comfortable and healthy. My Request If you're a remote worker like me, do your tele-brethren a favor: make your employer delighted with everything you do or produce. You may get laid off one day, you may get called back to an office to help \"drive collaboration and innovation\". But if you're kicking ass, you'll both know remote work wasn't getting in the way.","title":"the verdict is in: remote work ftw"},{"location":"posts/the-verdict-is-in/#the-cycle-of-life-and-death","text":"If you perform a google search for \"the death of remote work\", you'll be treated to a bevy of articles, spanning the course of multiple years. You're likely to notice something in those search results. A definitive cycle of acceptance and novelty, followed by pervasive FUD. The prevailing pattern seems to be as follows: A unique, limited population of pioneers demonstrate remote work is viable More interest is generated amongst job seekers for remote work Remote work seems to be on the path to widespread, general acceptance Larger employers start to embrace some variety of remote work High profile employer recalls remote work policy Sensationalist articles are published, declaring remote work \"dead\" Rinse and repeat Sadly, almost all those articles leave out an important piece of context: to a one, the high profile employers involved in the recall or total elimination of remote work policies did so in the shadow of a prolonged period of lackluster results and ceding market share to competitors. Most corporate leadership teams are not going to make any sweeping changes to such policies when things are going well. Why fix what's not broken? But when things turn south, a lot of management teams head straight for the \"micromanage employee time\" playbook. How do you know when they're using that playbook? Look for some generic statement about \"driving collaboration and innovation\". If that statement seems incredibly disingenuous, it's because it is. There's a couple bits to unpack here. First, collaboration and innovation are not mutually exclusive to being present in an office. The ubiquitousness of online communication and collaboration tools like Slack and WebEx means collaborating with co-workers is easier than ever. Not only that, now you have a logged, searchable record of any innovative ideas that are discussed. Secondly, just how much were employees, no matter their working location, involved in long-term, company-wide product strategy discussions? Investment discussions? Market strategy? These fall squarely on the shoulders of the c-suite. Brass-tacks: the company is struggling, and this is an easy lever to pull to appease stakeholders and investors. The next time you read remote work's eulogy, just be sure to look at the big picture, and really consider the context in which it's being presented.","title":"The Cycle of Life and Death"},{"location":"posts/the-verdict-is-in/#remote-work-does-not-make-you-a-better-communicator","text":"Real talk: if you're not already comfortable being a robust and prolific communicator, remote work will not fix that. I see a lot of comments from individuals who define themselves as \"socially awkward\", or uncomfortable with face-to-face interaction, or who believe remote work will suddenly make them better able to work with peers. Nope. You need to very comfortable with frequent interaction, including video calls/conferences. Remote workers need to bias on the side of over communicating. As a remote worker, simple physics dictates that you're going to lose a little bit of visibility and mind share when you don't have a physical presence in the office. You'll need to be compensating for that consistently. If you don't imagine yourself as someone who's comfortable communicating frequently with co-workers of varying levels of seniority and technical acumen, who can be available to lead, take ownership, schedule and run meetings, you're going to suffer as a remote worker.","title":"Remote Work Does Not Make You A Better Communicator"},{"location":"posts/the-verdict-is-in/#invest-in-the-tools","text":"I've spent close to $2500 on furniture and tools for my office. That may seem like a lot, but that's a drop in the bucket compared to the potential lifetime of medical bills from an RSI. You know what makes it easier to be at your desk for 8 to 10 hours a day, available, motivated, and energetic? A comfortable work environment. A 50, or even 100 or 200 dollar chair from Staples is not going to cut it. The antique wooden desk you pulled out of the attic? Made before ergonomics was really a thing. You can't legitmately pronounce remote work a failure, or not possible, if you don't invest up-front in the kinds of tools that ensure your comfort and health. A Jarvis Bamboo adjustable standing desk and a Herman Miller Aeron chair allow me to stay at my desk for hours with nary pain or stiffness. Do yourself a favor. If remote work is really a long-term commitment, commit to making sure it's comfortable and healthy.","title":"Invest In The Tools"},{"location":"posts/the-verdict-is-in/#my-request","text":"If you're a remote worker like me, do your tele-brethren a favor: make your employer delighted with everything you do or produce. You may get laid off one day, you may get called back to an office to help \"drive collaboration and innovation\". But if you're kicking ass, you'll both know remote work wasn't getting in the way.","title":"My Request"},{"location":"posts/welp-goals-are-hard/","text":"So yeah, as it turns out, goals are hard. A month ago, I set out some loosely defined goals for 2021 . How is it going, you ask? Well, this is the first post in the month since I wrote that post, the number of Twitter followers I have are exactly the same, and I haven't launched any sites or products. Not exactly a hot start to the year. A tweet I saw recently made me think that it might be more about \"systems\" instead of \"goals\": https://x.com/petecodes/status/1361598563525005312 (the whole thread is gold, and I highly recommend giving Pete a follow) So, rather than making some fixed goals, and feeling bad when I don't hit them, I think I should probably aim to do \"something\" each day, and only work to ensure it's productive and generally aligned to some \"end state\". For example: learning a new framework or tool isn't necessarily a goal, but choosing to learn something that might be used as part of a stack for a launched product or project is moving the needle, albeit in a roundabout way. Let's give systems a shot, and see how that works out.","title":"Welp, Goals Are Hard"},{"location":"posts/what-i-use/","text":"I've always been interested in what people \"use\" to carry out knowledge work. Input devices, IDEs, ergonomics(very important for a variety of reasons), scheduling, task management etc... I spend a lot of time poring over various threads and blog posts, looking for anything that I can integrate into my own workflows to potentially improve my productivity. Note that net-output doesn't have to be the only goal in-mind. Sometimes I just want to try something out that's piqued my intellectual curiosity(Emacs anyone?) This post is my small contribution to that pile. For those unfamiliar: I'm a remote worker, and perhaps needless to say a huge proponent of that paradigm for most knowledge work. In my opinion, ergonomics are the most important consideration for a remote worker, and merit serious investment. I've highlighted them as the first section. Ergonomics Chair - Herman Miller Aeron - I can sit in this chair for hours on end with zero pain or discomfort. Having a comfortable, adjustable ergonomic chair is important for maintaining good posture and avoiding long-term injury from prolonged periods of seating. It's going to be expensive(a new one is ~1200 USD), but it's absolutely worth it in the long term. You can probably find refurbished/renewed chairs for a significant discount for a minimum amount of effort. I was able to find a refurbished one locally for half the price. Desk - Jarvis Bamboo - Sit, Stand, Move, Repeat is the mantra of modern, healthy office work. Obviously they want to sell you expensive office furniture, but the health dangers of prolonged sitting are well known at this point. Once upon a time, the Bamboo was the highest-rated standing desk by the Wirecutter. The new king appears to be the Uplift , but the ultimate goal is to get you to change positions throughout the day. My Bamboo has served me well, and survived a move as well. It's slightly shaky at full extension, but overall it was a worthwhile investment. Monitor Arms - Amazon Basics Premium Single Monitor Stand - I could've opted for more expensive options from Humanscale or Ergotron, but these were good enough\u2122. The key is that you need to be able to adjust the height so that your eyes naturally rest at the top edge of your monitor's visible screen area(the ideal ergonomic position). Wrist Rests - IMAK Beaded Wrist Cushion , IMAK Keyboard Wrist Cushion - There is some debate as to whether wrist rests are helpful or detrimental . Did you know that you're actually supposed to rest the heel/palm of your hand on them, not your wrist? The key takeaway is there should be as straight a \"line\"(or plane) as possible, extending from your elbows to the tips of your fingers. You want to keep your wrists as straight as possible. If wrists rests help make that easier/more comfortable, I think it's worth it. The Rest of The Desk Computer - 2016 rMBP Pro 15in - Work-issued. I've been issued a Mac from every employer I've had since 2014. I've been generally lucky in avoiding some of the common issues that have plagued certain iterations. My home system is still a 2012-era Macbook Pro. I think Macs still provide the best combo of a Unix-like userland environment and functional UX. Longer-term, especially for home usage, I might consider migrating to a Linux laptop or workstation. Monitors - 2x U2715H Ultrasharp - These are several years old at this point, but they continue to serve me well. The Ultrasharp line of monitors have a great picture, and make reading text a joy(a fairly common use case in DevOps!). I've toyed with the idea of moving to a single, large widescreen like the 38in Ultrasharp , but I do enjoy the logical separation provided by two screens. Keyboard - Realforce TKL 55g Ivory - I love me some mechanical keyboards. If you find yourself typing on a cheap OEM mechanical rubber-dome piece of plastic, go out and get one right-now . Historically, I've used keyboard with Cherry MX Blue switches (click-clack). The Realforce is my first with Topre switches, and it is a joy to type on. Mouse - Logitech MX Mouse Wireless - It's wireless, and it has an ergonomic grip. It's been a great mouse for me, even though I'm constantly trying to use less of it by learning keyboard shortcuts. Laptop Dock - Plugable Thunderbolt 3 Dock - Even though I was issued a laptop, 99% of the time I'm working from my home office. I keep my laptop docked, and run through external monitors and input devices. There are probably newer options at this point, but the Plugable has worked well for me through 3 different models of Mac. Laptop Stand - TwelveSouth Bookarc - The Bookarc is a vertical Mac stand that comes with inserts to allow for various sizes of MBP. A vertical stand allows you to reclaim precious desk real-estate, and doesn't look half bad either. Headphones - Bose QC35 II Wireless Headphones - Decent, comfortable wireless headphones with noise canceling and a built-in mic are a must-have, especially for remote workers. I attend several meetings a week, and the sound quality of the mic is a marked improvement from my webcam. The listening experience is also second-to-none, and the noise canceling + good music can help get you in a state of flow faster. Core Software Tools IDE - VSCode - Once a die-hard Vim user , I've joined the VSCode camp. The amount of extensibility and customization available is incredible, and you get a lot of stuff out-of-the-box that would usually require extensive customization and scripting in a bare-bones text-editor like Vim. I still reach for Vim for quick editing sometimes, and have been learning Emacs on and off as well, but for day-to-day work, VSCode is my primary tool. Terminal Emulator - iTerm2 - If you do any work in a Terminal on the Mac, this is a must-have replacement for the default Terminal app. It's customizable, scriptable, searchable, works with Tmux... the list goes on. I discover new features and tricks constantly, and I'm always amazed with what it can do. There was a time when it seemed to have lost its way, and I considered Kitty , but those days appear to be past, and it runs great. Chat - Slack - The chat app of choice for most technical orgs, and quite a few public communities as well. It's not without issues, but overall it's great at what it does, and provides a great medium for remote workers to share in office communications. They claim it will replace email, and I think it's getting there. To-do - Apple Reminders - I've always struggled with upkeep on a to-do system. I've tried a few apps/tools, and I eventually settled on just using the native Reminders app(sorry Android users). It easily integrates with all my Apple devices, and can be shared with family members as well. An added benefit is I can use Siri to dictate items to the list, rather than having to type. Reducing friction is one of the major keys to implementing a new system or habit, so voice is a game changer in that regard. Calendar - Google Calendar - Concerns about Google having my data aside, GCal has served well over the years. My current workflow is to import them via Apple Calendar, so I can use Siri to manage events on my iOS devices. Email - GMail - I've had a GMail address for 15 years at this point. At the time, it was light-years ahead of everything else, and they've steadily added features since then. The dangers of consolidating personal data with any one corporate entity are top-of-mind these days, so I'm eager to see what the new generation of email clients/services like Hey bring. Other Useful Software Containers - Docker - That's right, plain ol' Docker containers. They're a great way to isolate development environments and workflows on your local workstation, with a minimal abstraction cost. RESTful Development - Insomnia - If you do any work with REST APIs, having a feature-rich client can save you hours of debugging pain. No more multi-line curl invocations! I've recently discovered this tool, and it's been a huge boon to my productivity, and I've barely scratched the surface of what it can do. Password Management - Lastpass - Having a secure way to generate and store randomized passwords is a must. Data breaches, unfortunately, have become a fact of life, and if you're using the same combo of birthday and your cat's name on 50 different sites, you're in trouble. Music/Podcasts - Spotify - The amount of music and content you get access to for 10 USD/mo is pretty incredible. I've recently paired it with the amazing discoverquickly.com tool to quickly generate ambient playlists for focus when I'm coding. PKM(Personal Knowledge Management) - notion.so - In the past, I've used software like Evernote and OneNote to keep \"notes\" about things. Inspired by systems like BASB , this time around I'm making a stronger effort to retain and analyze my various thoughts, ideas, experiences, and consumed content. Notion has a lot of powerful features(the tables are amazing), and will soon be adding an API as well. That constitutes about 98% of what I touch or use on a regular basis during any given day. As time passes, I may find new/better tools to replace what's currently here. Look for future posts to highlight interesting or useful things as I find them.","title":"What I Use"},{"location":"posts/what-i-use/#ergonomics","text":"Chair - Herman Miller Aeron - I can sit in this chair for hours on end with zero pain or discomfort. Having a comfortable, adjustable ergonomic chair is important for maintaining good posture and avoiding long-term injury from prolonged periods of seating. It's going to be expensive(a new one is ~1200 USD), but it's absolutely worth it in the long term. You can probably find refurbished/renewed chairs for a significant discount for a minimum amount of effort. I was able to find a refurbished one locally for half the price. Desk - Jarvis Bamboo - Sit, Stand, Move, Repeat is the mantra of modern, healthy office work. Obviously they want to sell you expensive office furniture, but the health dangers of prolonged sitting are well known at this point. Once upon a time, the Bamboo was the highest-rated standing desk by the Wirecutter. The new king appears to be the Uplift , but the ultimate goal is to get you to change positions throughout the day. My Bamboo has served me well, and survived a move as well. It's slightly shaky at full extension, but overall it was a worthwhile investment. Monitor Arms - Amazon Basics Premium Single Monitor Stand - I could've opted for more expensive options from Humanscale or Ergotron, but these were good enough\u2122. The key is that you need to be able to adjust the height so that your eyes naturally rest at the top edge of your monitor's visible screen area(the ideal ergonomic position). Wrist Rests - IMAK Beaded Wrist Cushion , IMAK Keyboard Wrist Cushion - There is some debate as to whether wrist rests are helpful or detrimental . Did you know that you're actually supposed to rest the heel/palm of your hand on them, not your wrist? The key takeaway is there should be as straight a \"line\"(or plane) as possible, extending from your elbows to the tips of your fingers. You want to keep your wrists as straight as possible. If wrists rests help make that easier/more comfortable, I think it's worth it.","title":"Ergonomics"},{"location":"posts/what-i-use/#the-rest-of-the-desk","text":"Computer - 2016 rMBP Pro 15in - Work-issued. I've been issued a Mac from every employer I've had since 2014. I've been generally lucky in avoiding some of the common issues that have plagued certain iterations. My home system is still a 2012-era Macbook Pro. I think Macs still provide the best combo of a Unix-like userland environment and functional UX. Longer-term, especially for home usage, I might consider migrating to a Linux laptop or workstation. Monitors - 2x U2715H Ultrasharp - These are several years old at this point, but they continue to serve me well. The Ultrasharp line of monitors have a great picture, and make reading text a joy(a fairly common use case in DevOps!). I've toyed with the idea of moving to a single, large widescreen like the 38in Ultrasharp , but I do enjoy the logical separation provided by two screens. Keyboard - Realforce TKL 55g Ivory - I love me some mechanical keyboards. If you find yourself typing on a cheap OEM mechanical rubber-dome piece of plastic, go out and get one right-now . Historically, I've used keyboard with Cherry MX Blue switches (click-clack). The Realforce is my first with Topre switches, and it is a joy to type on. Mouse - Logitech MX Mouse Wireless - It's wireless, and it has an ergonomic grip. It's been a great mouse for me, even though I'm constantly trying to use less of it by learning keyboard shortcuts. Laptop Dock - Plugable Thunderbolt 3 Dock - Even though I was issued a laptop, 99% of the time I'm working from my home office. I keep my laptop docked, and run through external monitors and input devices. There are probably newer options at this point, but the Plugable has worked well for me through 3 different models of Mac. Laptop Stand - TwelveSouth Bookarc - The Bookarc is a vertical Mac stand that comes with inserts to allow for various sizes of MBP. A vertical stand allows you to reclaim precious desk real-estate, and doesn't look half bad either. Headphones - Bose QC35 II Wireless Headphones - Decent, comfortable wireless headphones with noise canceling and a built-in mic are a must-have, especially for remote workers. I attend several meetings a week, and the sound quality of the mic is a marked improvement from my webcam. The listening experience is also second-to-none, and the noise canceling + good music can help get you in a state of flow faster.","title":"The Rest of The Desk"},{"location":"posts/what-i-use/#core-software-tools","text":"IDE - VSCode - Once a die-hard Vim user , I've joined the VSCode camp. The amount of extensibility and customization available is incredible, and you get a lot of stuff out-of-the-box that would usually require extensive customization and scripting in a bare-bones text-editor like Vim. I still reach for Vim for quick editing sometimes, and have been learning Emacs on and off as well, but for day-to-day work, VSCode is my primary tool. Terminal Emulator - iTerm2 - If you do any work in a Terminal on the Mac, this is a must-have replacement for the default Terminal app. It's customizable, scriptable, searchable, works with Tmux... the list goes on. I discover new features and tricks constantly, and I'm always amazed with what it can do. There was a time when it seemed to have lost its way, and I considered Kitty , but those days appear to be past, and it runs great. Chat - Slack - The chat app of choice for most technical orgs, and quite a few public communities as well. It's not without issues, but overall it's great at what it does, and provides a great medium for remote workers to share in office communications. They claim it will replace email, and I think it's getting there. To-do - Apple Reminders - I've always struggled with upkeep on a to-do system. I've tried a few apps/tools, and I eventually settled on just using the native Reminders app(sorry Android users). It easily integrates with all my Apple devices, and can be shared with family members as well. An added benefit is I can use Siri to dictate items to the list, rather than having to type. Reducing friction is one of the major keys to implementing a new system or habit, so voice is a game changer in that regard. Calendar - Google Calendar - Concerns about Google having my data aside, GCal has served well over the years. My current workflow is to import them via Apple Calendar, so I can use Siri to manage events on my iOS devices. Email - GMail - I've had a GMail address for 15 years at this point. At the time, it was light-years ahead of everything else, and they've steadily added features since then. The dangers of consolidating personal data with any one corporate entity are top-of-mind these days, so I'm eager to see what the new generation of email clients/services like Hey bring.","title":"Core Software Tools"},{"location":"posts/what-i-use/#other-useful-software","text":"Containers - Docker - That's right, plain ol' Docker containers. They're a great way to isolate development environments and workflows on your local workstation, with a minimal abstraction cost. RESTful Development - Insomnia - If you do any work with REST APIs, having a feature-rich client can save you hours of debugging pain. No more multi-line curl invocations! I've recently discovered this tool, and it's been a huge boon to my productivity, and I've barely scratched the surface of what it can do. Password Management - Lastpass - Having a secure way to generate and store randomized passwords is a must. Data breaches, unfortunately, have become a fact of life, and if you're using the same combo of birthday and your cat's name on 50 different sites, you're in trouble. Music/Podcasts - Spotify - The amount of music and content you get access to for 10 USD/mo is pretty incredible. I've recently paired it with the amazing discoverquickly.com tool to quickly generate ambient playlists for focus when I'm coding. PKM(Personal Knowledge Management) - notion.so - In the past, I've used software like Evernote and OneNote to keep \"notes\" about things. Inspired by systems like BASB , this time around I'm making a stronger effort to retain and analyze my various thoughts, ideas, experiences, and consumed content. Notion has a lot of powerful features(the tables are amazing), and will soon be adding an API as well. That constitutes about 98% of what I touch or use on a regular basis during any given day. As time passes, I may find new/better tools to replace what's currently here. Look for future posts to highlight interesting or useful things as I find them.","title":"Other Useful Software"}]}